{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY\n",
    "top_k_categorical_accuracy\n",
    "https://stackoverflow.com/questions/47887533/keras-convolution-along-samples\n",
    "https://keras.io/layers/wrappers/#timedistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "input_width = 160\n",
    "input_height = 100\n",
    "channels = 3\n",
    "class_number = 12\n",
    "data_path = \"D:\\\\Python\\\\Wormax_learn2\\\\preprocessed_data_local_notshuffled\\\\\"\n",
    "model_name = 'worm_transfer_single_6'\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers, models, optimizers\n",
    "import keras.backend as K\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=(input_height, input_width, channels))\n",
    "\n",
    "\n",
    "def actual_acc(y_true, y_pred):\n",
    "    return K.equal(K.argmax(y_pred), K.argmax(y_true))\n",
    "\n",
    "def convolution_feature_extractor(input_height, input_width):\n",
    "    model = models.Sequential()        \n",
    "    model.add(conv_base)   \n",
    "    return model\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(convolution_feature_extractor(input_height, input_width))   \n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(class_number, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[actual_acc])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 3, 5, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1966336   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 16,684,108\n",
      "Trainable params: 1,969,420\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = False\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little prepocessing\n",
    "from math import atan2, pi\n",
    "\n",
    "def get_angle(x, y):\n",
    "    return atan2(y, x)\n",
    "\n",
    "def get_direction(x, y, n_classes = 12):\n",
    "    return round(get_angle(x, y)/2/pi*n_classes)%n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Training and validation\n",
    "data_ratio = 0.7\n",
    "\n",
    "def generator(data_dir, num_classes, role, shuffle=True, batch_size=128):\n",
    "    \n",
    "    listdir = []\n",
    "    listdir = filter(lambda x: os.path.isfile, os.listdir(data_dir))\n",
    "    listdir = np.array(list(listdir))\n",
    "    random.shuffle(listdir)\n",
    "    \n",
    "    #print('Found {} files for {}'.format(len(listdir), role))\n",
    "    \n",
    "    file_i = 0\n",
    "    while 1:\n",
    "        data = np.load(data_dir + listdir[file_i])\n",
    "        file_i = (file_i+1) if file_i+1<len(listdir) else 0\n",
    "        \n",
    "        if role == 'train':\n",
    "            data = data[:int(round(len(data)*data_ratio))]\n",
    "        elif role == 'validation':\n",
    "            data = data[int(round(len(data)*data_ratio)):]\n",
    "        else:\n",
    "            raise 'bad role parameter'\n",
    "        \n",
    "        # Generating y\n",
    "        data_targets = np.zeros((len(data)-1, 12))\n",
    "        for i in range(len(data)-1):\n",
    "            data_targets[i] = np.array(to_categorical(get_direction(*data[i+1][1][:2]), num_classes=num_classes))\n",
    "        data = data[:-1]\n",
    "        \n",
    "        # Only X\n",
    "        data_features = data[:,0]\n",
    "        \n",
    "        indexes = np.arange(len(data_features))\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "        for i in range(0, len(data)-1-batch_size, batch_size):\n",
    "            samples = data_features.take(indexes[i:i+batch_size],axis=0)\n",
    "            targets = data_targets.take(indexes[i:i+batch_size],axis=0)\n",
    "            \n",
    "            # Weird reshape cuz bug # actually not bug, initial data array is not tensor(idk how to fix)\n",
    "            samples2 = np.zeros((batch_size, input_height, input_width, channels))\n",
    "            for j, sample in enumerate(samples):\n",
    "                samples2[j] = sample\n",
    "            \n",
    "            # will not work without this\n",
    "            samples2 = samples2 / 255\n",
    "            yield samples2, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count class instances count for balancing\n",
    "if False:\n",
    "    i = 0\n",
    "    classes = np.zeros((class_number))\n",
    "    for samples, targets in generator(data_path, class_number, 'train', batch_size=1024):\n",
    "        for j in targets:\n",
    "            classes += j\n",
    "        i += 1\n",
    "        if i == 1000:\n",
    "            break\n",
    "    classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 160, 3)\n",
      "(8, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_generator = generator(data_path, class_number, 'train', batch_size=8)\n",
    "validation_generator = generator(data_path, class_number, 'validation', batch_size=8)\n",
    "\n",
    "print(next(train_generator)[0].shape)\n",
    "print(next(train_generator)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### tensorboard --logdir=D:\\Python\\Keras\\Wormax\\log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100/100 [==============================] - 15s 150ms/step - loss: 4.4867 - actual_acc: 0.1450 - val_loss: 2.6455 - val_actual_acc: 0.0988\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 4.0147 - actual_acc: 0.1988 - val_loss: 2.4028 - val_actual_acc: 0.2006\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.8751 - actual_acc: 0.2275 - val_loss: 2.5757 - val_actual_acc: 0.1105\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.7550 - actual_acc: 0.2587 - val_loss: 2.5961 - val_actual_acc: 0.1134\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.3977 - actual_acc: 0.3062 - val_loss: 2.4881 - val_actual_acc: 0.1308\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.3266 - actual_acc: 0.3263 - val_loss: 2.7213 - val_actual_acc: 0.0959\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.2130 - actual_acc: 0.3787 - val_loss: 2.3848 - val_actual_acc: 0.1802\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 3.2006 - actual_acc: 0.3612 - val_loss: 2.4553 - val_actual_acc: 0.1541\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 15s 150ms/step - loss: 3.2287 - actual_acc: 0.3400 - val_loss: 2.5697 - val_actual_acc: 0.1337\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.5844 - actual_acc: 0.2725 - val_loss: 2.6735 - val_actual_acc: 0.1279\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.0420 - actual_acc: 0.3725 - val_loss: 2.6252 - val_actual_acc: 0.1076\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.9057 - actual_acc: 0.4062 - val_loss: 2.7575 - val_actual_acc: 0.1192\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.8774 - actual_acc: 0.4012 - val_loss: 2.8574 - val_actual_acc: 0.1279\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.7531 - actual_acc: 0.4312 - val_loss: 2.7435 - val_actual_acc: 0.1512\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 2.6129 - actual_acc: 0.4475 - val_loss: 2.4116 - val_actual_acc: 0.2122\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.4704 - actual_acc: 0.4975 - val_loss: 2.5048 - val_actual_acc: 0.1919\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.3757 - actual_acc: 0.5250 - val_loss: 2.4449 - val_actual_acc: 0.1686\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 3.1999 - actual_acc: 0.3700 - val_loss: 2.3482 - val_actual_acc: 0.2064\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.4114 - actual_acc: 0.2900 - val_loss: 2.3571 - val_actual_acc: 0.2238\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.9282 - actual_acc: 0.3850 - val_loss: 2.2958 - val_actual_acc: 0.1890\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.7386 - actual_acc: 0.4338 - val_loss: 2.2054 - val_actual_acc: 0.2500\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 2.4926 - actual_acc: 0.4787 - val_loss: 2.4604 - val_actual_acc: 0.1919\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.3963 - actual_acc: 0.5050 - val_loss: 2.5276 - val_actual_acc: 0.2267\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.3249 - actual_acc: 0.5363 - val_loss: 2.5333 - val_actual_acc: 0.1773\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 2.1520 - actual_acc: 0.5413 - val_loss: 2.7463 - val_actual_acc: 0.1686\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 1.9776 - actual_acc: 0.5800 - val_loss: 2.7153 - val_actual_acc: 0.1570\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 21s 211ms/step - loss: 3.3563 - actual_acc: 0.3200 - val_loss: 2.3913 - val_actual_acc: 0.1686\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.9946 - actual_acc: 0.3750 - val_loss: 2.4183 - val_actual_acc: 0.2035\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.8835 - actual_acc: 0.4175 - val_loss: 2.4861 - val_actual_acc: 0.2180\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.4964 - actual_acc: 0.4813 - val_loss: 2.3705 - val_actual_acc: 0.2500\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.5727 - actual_acc: 0.4713 - val_loss: 2.2842 - val_actual_acc: 0.2151\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.3263 - actual_acc: 0.5100 - val_loss: 2.5579 - val_actual_acc: 0.1628\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.1840 - actual_acc: 0.5487 - val_loss: 2.3875 - val_actual_acc: 0.2733\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 2.0617 - actual_acc: 0.5863 - val_loss: 2.6495 - val_actual_acc: 0.1366\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 2.0530 - actual_acc: 0.6012 - val_loss: 2.6721 - val_actual_acc: 0.1715\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 4.0078 - actual_acc: 0.1938 - val_loss: 2.3890 - val_actual_acc: 0.1279\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.5773 - actual_acc: 0.2213 - val_loss: 2.4137 - val_actual_acc: 0.1250\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.5532 - actual_acc: 0.2500 - val_loss: 2.3384 - val_actual_acc: 0.2151\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.5832 - actual_acc: 0.2800 - val_loss: 2.4319 - val_actual_acc: 0.1657\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.2716 - actual_acc: 0.3188 - val_loss: 2.5217 - val_actual_acc: 0.1221\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 15s 145ms/step - loss: 3.2086 - actual_acc: 0.3500 - val_loss: 2.3590 - val_actual_acc: 0.2180\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.1817 - actual_acc: 0.3463 - val_loss: 2.4271 - val_actual_acc: 0.1977\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.9657 - actual_acc: 0.3825 - val_loss: 2.5969 - val_actual_acc: 0.1831\n",
      "Epoch 44/500\n",
      "100/100 [==============================] - 15s 149ms/step - loss: 3.2449 - actual_acc: 0.3287 - val_loss: 2.3387 - val_actual_acc: 0.1541\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.7163 - actual_acc: 0.2400 - val_loss: 2.5471 - val_actual_acc: 0.1134\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.4986 - actual_acc: 0.2888 - val_loss: 2.2935 - val_actual_acc: 0.2180\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 3.2541 - actual_acc: 0.3375 - val_loss: 2.3507 - val_actual_acc: 0.2035\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.0904 - actual_acc: 0.3762 - val_loss: 2.5102 - val_actual_acc: 0.1948\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.0576 - actual_acc: 0.3550 - val_loss: 2.4539 - val_actual_acc: 0.1890\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.9733 - actual_acc: 0.3650 - val_loss: 2.5938 - val_actual_acc: 0.1860\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.7753 - actual_acc: 0.4000 - val_loss: 2.4768 - val_actual_acc: 0.2297\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.8163 - actual_acc: 0.4363 - val_loss: 2.5635 - val_actual_acc: 0.2035\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 3.3937 - actual_acc: 0.3000 - val_loss: 2.2506 - val_actual_acc: 0.2297\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.5001 - actual_acc: 0.2625 - val_loss: 2.3230 - val_actual_acc: 0.1948\n",
      "Epoch 55/500\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 3.5365 - actual_acc: 0.3050 - val_loss: 2.3699 - val_actual_acc: 0.2384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.1920 - actual_acc: 0.3188 - val_loss: 2.5648 - val_actual_acc: 0.1686\n",
      "Epoch 57/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.1743 - actual_acc: 0.3250 - val_loss: 2.7291 - val_actual_acc: 0.1250\n",
      "Epoch 58/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.1490 - actual_acc: 0.3537 - val_loss: 2.6718 - val_actual_acc: 0.1483\n",
      "Epoch 59/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.1058 - actual_acc: 0.3762 - val_loss: 2.7497 - val_actual_acc: 0.1453\n",
      "Epoch 60/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.0072 - actual_acc: 0.3825 - val_loss: 2.8496 - val_actual_acc: 0.1453\n",
      "Epoch 61/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.7777 - actual_acc: 0.3937 - val_loss: 2.5331 - val_actual_acc: 0.1919\n",
      "Epoch 62/500\n",
      "100/100 [==============================] - 22s 217ms/step - loss: 3.8682 - actual_acc: 0.2000 - val_loss: 2.3605 - val_actual_acc: 0.2064\n",
      "Epoch 63/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.7358 - actual_acc: 0.2037 - val_loss: 2.3591 - val_actual_acc: 0.2006\n",
      "Epoch 64/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.5670 - actual_acc: 0.2500 - val_loss: 2.3084 - val_actual_acc: 0.1802\n",
      "Epoch 65/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.3999 - actual_acc: 0.3025 - val_loss: 2.2686 - val_actual_acc: 0.1919\n",
      "Epoch 66/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.3579 - actual_acc: 0.3275 - val_loss: 2.1856 - val_actual_acc: 0.2413\n",
      "Epoch 67/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.2965 - actual_acc: 0.3250 - val_loss: 2.2768 - val_actual_acc: 0.2035\n",
      "Epoch 68/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.1167 - actual_acc: 0.3450 - val_loss: 2.1233 - val_actual_acc: 0.2500\n",
      "Epoch 69/500\n",
      "100/100 [==============================] - 13s 131ms/step - loss: 3.0545 - actual_acc: 0.3537 - val_loss: 2.6022 - val_actual_acc: 0.1860\n",
      "Epoch 70/500\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 3.0131 - actual_acc: 0.3837 - val_loss: 2.7559 - val_actual_acc: 0.1337\n",
      "Epoch 71/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.7502 - actual_acc: 0.2162 - val_loss: 2.4715 - val_actual_acc: 0.1686\n",
      "Epoch 72/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.6089 - actual_acc: 0.2213 - val_loss: 2.5630 - val_actual_acc: 0.1308\n",
      "Epoch 73/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.5131 - actual_acc: 0.2500 - val_loss: 2.6782 - val_actual_acc: 0.1744\n",
      "Epoch 74/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.4093 - actual_acc: 0.2775 - val_loss: 2.6340 - val_actual_acc: 0.1366\n",
      "Epoch 75/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.3479 - actual_acc: 0.2938 - val_loss: 2.7258 - val_actual_acc: 0.1424\n",
      "Epoch 76/500\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 3.2551 - actual_acc: 0.3225 - val_loss: 2.5080 - val_actual_acc: 0.1948\n",
      "Epoch 77/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.0460 - actual_acc: 0.3537 - val_loss: 2.1510 - val_actual_acc: 0.2558\n",
      "Epoch 78/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.9948 - actual_acc: 0.3537 - val_loss: 2.2521 - val_actual_acc: 0.2297\n",
      "Epoch 79/500\n",
      "100/100 [==============================] - 15s 147ms/step - loss: 3.2325 - actual_acc: 0.3250 - val_loss: 2.3045 - val_actual_acc: 0.1831\n",
      "Epoch 80/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.3407 - actual_acc: 0.3125 - val_loss: 2.2499 - val_actual_acc: 0.2180\n",
      "Epoch 81/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.2266 - actual_acc: 0.3250 - val_loss: 2.2495 - val_actual_acc: 0.1977\n",
      "Epoch 82/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.9829 - actual_acc: 0.3438 - val_loss: 2.4049 - val_actual_acc: 0.1773\n",
      "Epoch 83/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 2.8650 - actual_acc: 0.3925 - val_loss: 2.2890 - val_actual_acc: 0.2820\n",
      "Epoch 84/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.6757 - actual_acc: 0.4263 - val_loss: 2.6122 - val_actual_acc: 0.1657\n",
      "Epoch 85/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.6365 - actual_acc: 0.4400 - val_loss: 2.4972 - val_actual_acc: 0.2326\n",
      "Epoch 86/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.6035 - actual_acc: 0.4350 - val_loss: 2.6293 - val_actual_acc: 0.2267\n",
      "Epoch 87/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.4416 - actual_acc: 0.4800 - val_loss: 2.8871 - val_actual_acc: 0.1802\n",
      "Epoch 88/500\n",
      "100/100 [==============================] - 15s 151ms/step - loss: 3.2855 - actual_acc: 0.3488 - val_loss: 2.4916 - val_actual_acc: 0.2006\n",
      "Epoch 89/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.3081 - actual_acc: 0.3162 - val_loss: 2.3821 - val_actual_acc: 0.1308\n",
      "Epoch 90/500\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 3.1553 - actual_acc: 0.3362 - val_loss: 2.4785 - val_actual_acc: 0.1890\n",
      "Epoch 91/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.8931 - actual_acc: 0.3638 - val_loss: 2.3239 - val_actual_acc: 0.2558\n",
      "Epoch 92/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.8695 - actual_acc: 0.4125 - val_loss: 2.3696 - val_actual_acc: 0.2238\n",
      "Epoch 93/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.6060 - actual_acc: 0.4738 - val_loss: 2.4885 - val_actual_acc: 0.2384\n",
      "Epoch 94/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.4381 - actual_acc: 0.5000 - val_loss: 2.4449 - val_actual_acc: 0.2616\n",
      "Epoch 95/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.4967 - actual_acc: 0.4750 - val_loss: 2.3817 - val_actual_acc: 0.2238\n",
      "Epoch 96/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.5038 - actual_acc: 0.4963 - val_loss: 2.4841 - val_actual_acc: 0.2587\n",
      "Epoch 97/500\n",
      "100/100 [==============================] - 22s 218ms/step - loss: 3.4837 - actual_acc: 0.2462 - val_loss: 2.3665 - val_actual_acc: 0.1715\n",
      "Epoch 98/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 3.0711 - actual_acc: 0.3563 - val_loss: 2.3014 - val_actual_acc: 0.2035\n",
      "Epoch 99/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.9220 - actual_acc: 0.3600 - val_loss: 2.2577 - val_actual_acc: 0.2122\n",
      "Epoch 100/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.8726 - actual_acc: 0.3825 - val_loss: 2.3933 - val_actual_acc: 0.1599\n",
      "Epoch 101/500\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 2.5513 - actual_acc: 0.4363 - val_loss: 2.3377 - val_actual_acc: 0.2442\n",
      "Epoch 102/500\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.4261 - actual_acc: 0.4684"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 1.0,\n",
    "                 1: 1.62,\n",
    "                 2: 2.68,\n",
    "                 3: 3.36,\n",
    "                 4: 2.51,\n",
    "                 5: 1.53,\n",
    "                 6: 1.0,\n",
    "                 7: 1.37,\n",
    "                 8: 2.16,\n",
    "                 9: 2.61,\n",
    "                 10: 2.04,\n",
    "                 11: 1.3}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir\\\\' + model_name\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_name + '.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5,                              \n",
    "        patience=70, \n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "steps_per_epoch = 100\n",
    "history = model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=500,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=int(round(steps_per_epoch/data_ratio*(1-data_ratio))),\n",
    "                            shuffle=True,\n",
    "                            class_weight=class_weight,\n",
    "                            callbacks=callbacks\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
