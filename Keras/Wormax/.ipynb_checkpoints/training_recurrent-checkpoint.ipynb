{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY\n",
    "top_k_categorical_accuracy\n",
    "https://stackoverflow.com/questions/47887533/keras-convolution-along-samples\n",
    "https://keras.io/layers/wrappers/#timedistributed\n",
    "### !Try target position time (current or next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "input_width = 160\n",
    "input_height = 100\n",
    "sequence_size = 3\n",
    "class_number = 12\n",
    "data_path = \"D:\\\\Python\\\\Wormax_learn2\\\\preprocessed_data_local_notshuffled\\\\\"\n",
    "model_name = 'worm_sequence3_9_adam'\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import models, optimizers, layers\n",
    "import keras.backend as K\n",
    "from keras.applications import Xception\n",
    "from keras.layers import TimeDistributed, Conv2D, Dropout, LSTM, Dense, MaxPooling2D, Flatten, GRU\n",
    "\n",
    "def actual_acc(y_true, y_pred):\n",
    "    return K.equal(K.argmax(y_pred), K.argmax(y_true))\n",
    "\n",
    "def convolution_feature_extractor(input_height, input_width):\n",
    "    model = models.Sequential()        \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(input_height, input_width, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.summary()    \n",
    "    return model\n",
    "\n",
    "def define_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(TimeDistributed(\n",
    "            convolution_feature_extractor(input_height, input_width),\n",
    "            input_shape=(None, input_height, input_width, 3)\n",
    "            ))\n",
    "\n",
    "    model.add(GRU(128, return_sequences=True))#, dropout=0.5))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dense(class_number, activation='softmax'))    \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[actual_acc])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 158, 32)       896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 79, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 77, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 16, 128)        147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 240,832\n",
      "Trainable params: 240,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, None, 4096)        240832    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         1622400   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 1,963,468\n",
      "Trainable params: 1,963,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little prepocessing\n",
    "from math import atan2, pi\n",
    "\n",
    "def get_angle(x, y):\n",
    "    return atan2(y, x)\n",
    "\n",
    "def get_direction(x, y, n_classes = 12):\n",
    "    return round(get_angle(x, y)/2/pi*n_classes)%n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Training and validation\n",
    "data_ratio = 0.7\n",
    "            \n",
    "# Generator done for not to overflow MEM\n",
    "# holds one data file for every instance(train and validation)\n",
    "def generator(data_dir, sequence_size, num_classes, role, batch_size=128, shuffle=True):\n",
    "    \n",
    "    listdir = []\n",
    "    listdir = filter(lambda x: os.path.isfile, os.listdir(data_dir))\n",
    "    listdir = np.array(list(listdir))\n",
    "    random.shuffle(listdir)\n",
    "    \n",
    "    #print('Found {} files for {}'.format(len(listdir), role))\n",
    "    \n",
    "    file_i = 0\n",
    "    while 1:\n",
    "        data = np.load(data_dir + listdir[file_i])\n",
    "        file_i = (file_i+1) if file_i+1<len(listdir) else 0\n",
    "        \n",
    "        if role == 'train':\n",
    "            data = data[:int(round(len(data)*data_ratio))]\n",
    "        elif role == 'validation':\n",
    "            data = data[int(round(len(data)*data_ratio)):]\n",
    "        else:\n",
    "            raise TypeError('bad role parameter')\n",
    "        \n",
    "        indexes = np.arange(len(data) - sequence_size)        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "        for i in range(0, len(indexes), batch_size):\n",
    "            samples = np.zeros((batch_size, sequence_size, input_height, input_width, 3))\n",
    "            targets = np.zeros((batch_size, num_classes))\n",
    "            for j, index in enumerate(indexes[i:i + batch_size]):\n",
    "                # some issue with shapes(dummy reshape)\n",
    "                sample = np.zeros((sequence_size, input_height, input_width, 3))\n",
    "                for k, dt in enumerate(data[index:index + sequence_size, 0]):\n",
    "                    sample[k] = dt\n",
    "                samples[j] = sample\n",
    "                targets[j] = to_categorical(get_direction(*data[index + sequence_size][1][:2]), num_classes=num_classes)\n",
    "            \n",
    "            # will not work without this\n",
    "            samples = samples / 255\n",
    "            \n",
    "            yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10, 100, 160, 3)\n",
      "(10, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_generator = generator(data_path, sequence_size, class_number, 'train', batch_size=10)\n",
    "validation_generator = generator(data_path, sequence_size, class_number, 'validation', batch_size=10)\n",
    "\n",
    "print(next(train_generator)[0].shape)\n",
    "print(next(train_generator)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### tensorboard --logdir=D:\\Python\\Keras\\Wormax\\log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 4.3185 - actual_acc: 0.0790 - val_loss: 2.5564 - val_actual_acc: 0.0442\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 4.2194 - actual_acc: 0.1050 - val_loss: 2.5349 - val_actual_acc: 0.0791\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 3.7536 - actual_acc: 0.2140 - val_loss: 2.5865 - val_actual_acc: 0.1558\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 18s 185ms/step - loss: 3.5361 - actual_acc: 0.2720 - val_loss: 2.5735 - val_actual_acc: 0.1512\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 2.9325 - actual_acc: 0.4130 - val_loss: 2.7384 - val_actual_acc: 0.1791\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 31s 308ms/step - loss: 2.5296 - actual_acc: 0.5010 - val_loss: 2.8208 - val_actual_acc: 0.1605\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 2.1977 - actual_acc: 0.5580 - val_loss: 2.9939 - val_actual_acc: 0.1349\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 23s 232ms/step - loss: 3.9460 - actual_acc: 0.2300 - val_loss: 2.4187 - val_actual_acc: 0.1953\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 23s 232ms/step - loss: 3.1489 - actual_acc: 0.4030 - val_loss: 2.4980 - val_actual_acc: 0.1791\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 23s 235ms/step - loss: 2.7807 - actual_acc: 0.4640 - val_loss: 2.4791 - val_actual_acc: 0.2000\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 19s 185ms/step - loss: 2.4453 - actual_acc: 0.5200 - val_loss: 2.7557 - val_actual_acc: 0.1767\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 2.2735 - actual_acc: 0.5770 - val_loss: 2.7025 - val_actual_acc: 0.1930\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.0307 - actual_acc: 0.6020 - val_loss: 2.8268 - val_actual_acc: 0.1814\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 1.9507 - actual_acc: 0.6310 - val_loss: 2.6473 - val_actual_acc: 0.2349\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 3.2491 - actual_acc: 0.3720 - val_loss: 2.4650 - val_actual_acc: 0.2256\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 2.4126 - actual_acc: 0.5320 - val_loss: 2.8064 - val_actual_acc: 0.1767\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 27s 271ms/step - loss: 1.9894 - actual_acc: 0.6090 - val_loss: 2.7337 - val_actual_acc: 0.1791\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.6842 - actual_acc: 0.6810 - val_loss: 3.0261 - val_actual_acc: 0.1605\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 1.5463 - actual_acc: 0.6910 - val_loss: 3.2740 - val_actual_acc: 0.1651\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 22s 217ms/step - loss: 1.3199 - actual_acc: 0.7290 - val_loss: 3.3077 - val_actual_acc: 0.1628\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 22s 218ms/step - loss: 1.4845 - actual_acc: 0.7010 - val_loss: 3.1408 - val_actual_acc: 0.1535\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 3.9013 - actual_acc: 0.2390 - val_loss: 2.4512 - val_actual_acc: 0.1651\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 3.1746 - actual_acc: 0.3510 - val_loss: 2.5519 - val_actual_acc: 0.1419\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 2.7708 - actual_acc: 0.4670 - val_loss: 2.4352 - val_actual_acc: 0.1558\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 2.6247 - actual_acc: 0.4870 - val_loss: 2.6915 - val_actual_acc: 0.1581\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.2624 - actual_acc: 0.5480 - val_loss: 2.4992 - val_actual_acc: 0.1953\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 2.1919 - actual_acc: 0.5310 - val_loss: 2.5087 - val_actual_acc: 0.1744\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 24s 245ms/step - loss: 2.2450 - actual_acc: 0.5720 - val_loss: 3.1882 - val_actual_acc: 0.1372\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 3.4413 - actual_acc: 0.3120 - val_loss: 2.2733 - val_actual_acc: 0.2279\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 22s 220ms/step - loss: 2.7497 - actual_acc: 0.4260 - val_loss: 2.4147 - val_actual_acc: 0.1930\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 2.2956 - actual_acc: 0.5550 - val_loss: 2.5295 - val_actual_acc: 0.1558\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 37s 369ms/step - loss: 1.9753 - actual_acc: 0.6290 - val_loss: 2.6116 - val_actual_acc: 0.2070\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 37s 368ms/step - loss: 1.7679 - actual_acc: 0.6260 - val_loss: 2.7368 - val_actual_acc: 0.1977\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 1.4933 - actual_acc: 0.6840 - val_loss: 3.0016 - val_actual_acc: 0.1767\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 1.8691 - actual_acc: 0.6300 - val_loss: 2.6955 - val_actual_acc: 0.2163\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 3.4681 - actual_acc: 0.3160 - val_loss: 2.1073 - val_actual_acc: 0.2767\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 2.4756 - actual_acc: 0.4960 - val_loss: 2.2410 - val_actual_acc: 0.2860\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 20s 200ms/step - loss: 2.2754 - actual_acc: 0.5210 - val_loss: 2.2079 - val_actual_acc: 0.3047\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 20s 200ms/step - loss: 1.8510 - actual_acc: 0.6150 - val_loss: 2.2618 - val_actual_acc: 0.2628\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 1.6738 - actual_acc: 0.6610 - val_loss: 2.6210 - val_actual_acc: 0.2744\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 1.5765 - actual_acc: 0.6740 - val_loss: 2.8695 - val_actual_acc: 0.1860\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.6395 - actual_acc: 0.6860 - val_loss: 2.6816 - val_actual_acc: 0.2442\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 3.4896 - actual_acc: 0.2950 - val_loss: 2.2989 - val_actual_acc: 0.1860\n",
      "Epoch 44/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 2.9237 - actual_acc: 0.3910 - val_loss: 2.3295 - val_actual_acc: 0.2209\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.7669 - actual_acc: 0.4390 - val_loss: 2.4139 - val_actual_acc: 0.2233\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 2.5030 - actual_acc: 0.4650 - val_loss: 2.5050 - val_actual_acc: 0.2186\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 2.2567 - actual_acc: 0.5280 - val_loss: 2.4719 - val_actual_acc: 0.2512\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 2.0771 - actual_acc: 0.5580 - val_loss: 2.6595 - val_actual_acc: 0.2395\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 2.1879 - actual_acc: 0.5350 - val_loss: 2.6129 - val_actual_acc: 0.2233\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 3.3450 - actual_acc: 0.3330 - val_loss: 2.2742 - val_actual_acc: 0.2558\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 2.6764 - actual_acc: 0.4570 - val_loss: 2.3927 - val_actual_acc: 0.2000\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 2.3383 - actual_acc: 0.5090 - val_loss: 2.5575 - val_actual_acc: 0.2372\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 2.1992 - actual_acc: 0.5560 - val_loss: 2.6951 - val_actual_acc: 0.2023\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 2.4755 - actual_acc: 0.5090 - val_loss: 2.4738 - val_actual_acc: 0.2186\n",
      "Epoch 55/500\n",
      "100/100 [==============================] - 19s 187ms/step - loss: 3.4484 - actual_acc: 0.3290 - val_loss: 2.5065 - val_actual_acc: 0.1977\n",
      "Epoch 56/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.7822 - actual_acc: 0.4500 - val_loss: 2.5461 - val_actual_acc: 0.1953\n",
      "Epoch 57/500\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 2.3845 - actual_acc: 0.5150 - val_loss: 2.4554 - val_actual_acc: 0.1860\n",
      "Epoch 58/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 2.1416 - actual_acc: 0.5760 - val_loss: 2.6235 - val_actual_acc: 0.2233\n",
      "Epoch 59/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 1.9927 - actual_acc: 0.6130 - val_loss: 2.6972 - val_actual_acc: 0.2651\n",
      "Epoch 60/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 1.9777 - actual_acc: 0.5970 - val_loss: 2.6719 - val_actual_acc: 0.2302\n",
      "Epoch 61/500\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 2.2372 - actual_acc: 0.5670 - val_loss: 2.3690 - val_actual_acc: 0.2256\n",
      "Epoch 62/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 3.0854 - actual_acc: 0.3730 - val_loss: 2.2736 - val_actual_acc: 0.2651\n",
      "Epoch 63/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 2.1838 - actual_acc: 0.5780 - val_loss: 2.4405 - val_actual_acc: 0.2605\n",
      "Epoch 64/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.9756 - actual_acc: 0.6060 - val_loss: 2.6132 - val_actual_acc: 0.2302\n",
      "Epoch 65/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.7176 - actual_acc: 0.6670 - val_loss: 2.6616 - val_actual_acc: 0.1791\n",
      "Epoch 66/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.6424 - actual_acc: 0.6820 - val_loss: 2.7838 - val_actual_acc: 0.2256\n",
      "Epoch 67/500\n",
      "100/100 [==============================] - 28s 276ms/step - loss: 1.5138 - actual_acc: 0.6930 - val_loss: 2.7385 - val_actual_acc: 0.1930\n",
      "Epoch 68/500\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 2.0126 - actual_acc: 0.5890 - val_loss: 2.3698 - val_actual_acc: 0.2233\n",
      "Epoch 69/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.6492 - actual_acc: 0.4630 - val_loss: 2.2766 - val_actual_acc: 0.2163\n",
      "Epoch 70/500\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 2.0847 - actual_acc: 0.5770 - val_loss: 2.1892 - val_actual_acc: 0.2628\n",
      "Epoch 71/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 1.8918 - actual_acc: 0.5960 - val_loss: 2.4967 - val_actual_acc: 0.2000\n",
      "Epoch 72/500\n",
      "100/100 [==============================] - 18s 182ms/step - loss: 1.4960 - actual_acc: 0.6880 - val_loss: 2.7564 - val_actual_acc: 0.1721\n",
      "Epoch 73/500\n",
      "100/100 [==============================] - 25s 245ms/step - loss: 1.4690 - actual_acc: 0.6960 - val_loss: 2.6181 - val_actual_acc: 0.2767\n",
      "Epoch 74/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.2214 - actual_acc: 0.7430 - val_loss: 2.5477 - val_actual_acc: 0.2953\n",
      "Epoch 75/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.8759 - actual_acc: 0.6330 - val_loss: 2.6659 - val_actual_acc: 0.1791\n",
      "Epoch 76/500\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 2.9072 - actual_acc: 0.4170 - val_loss: 2.0986 - val_actual_acc: 0.3442\n",
      "Epoch 77/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.2474 - actual_acc: 0.5370 - val_loss: 2.4078 - val_actual_acc: 0.2605\n",
      "Epoch 78/500\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 1.9833 - actual_acc: 0.5860 - val_loss: 2.2175 - val_actual_acc: 0.3279\n",
      "Epoch 79/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.7341 - actual_acc: 0.6320 - val_loss: 2.0718 - val_actual_acc: 0.3744\n",
      "Epoch 80/500\n",
      "100/100 [==============================] - 18s 185ms/step - loss: 1.5359 - actual_acc: 0.6880 - val_loss: 2.0607 - val_actual_acc: 0.3791\n",
      "Epoch 81/500\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.4962 - actual_acc: 0.6890 - val_loss: 2.0291 - val_actual_acc: 0.3884\n",
      "Epoch 82/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 2.1203 - actual_acc: 0.5600 - val_loss: 2.1091 - val_actual_acc: 0.3372\n",
      "Epoch 83/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 3.3169 - actual_acc: 0.3410 - val_loss: 1.9492 - val_actual_acc: 0.3512\n",
      "Epoch 84/500\n",
      "100/100 [==============================] - 28s 276ms/step - loss: 2.8216 - actual_acc: 0.4230 - val_loss: 2.3223 - val_actual_acc: 0.2302\n",
      "Epoch 85/500\n",
      "100/100 [==============================] - 18s 185ms/step - loss: 2.6072 - actual_acc: 0.4400 - val_loss: 2.3175 - val_actual_acc: 0.2256\n",
      "Epoch 86/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.3355 - actual_acc: 0.5110 - val_loss: 2.3601 - val_actual_acc: 0.2070\n",
      "Epoch 87/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 1.9810 - actual_acc: 0.5950 - val_loss: 2.5670 - val_actual_acc: 0.2023\n",
      "Epoch 88/500\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 1.9303 - actual_acc: 0.5900 - val_loss: 2.6756 - val_actual_acc: 0.1791\n",
      "Epoch 89/500\n",
      "100/100 [==============================] - 40s 402ms/step - loss: 2.6280 - actual_acc: 0.4810 - val_loss: 2.3870 - val_actual_acc: 0.2116\n",
      "Epoch 90/500\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 3.3475 - actual_acc: 0.2990 - val_loss: 2.3696 - val_actual_acc: 0.2279\n",
      "Epoch 91/500\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 3.0277 - actual_acc: 0.3820 - val_loss: 2.2477 - val_actual_acc: 0.2581\n",
      "Epoch 92/500\n",
      "100/100 [==============================] - 34s 339ms/step - loss: 2.6477 - actual_acc: 0.4690 - val_loss: 2.4067 - val_actual_acc: 0.2535\n",
      "Epoch 93/500\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 2.4477 - actual_acc: 0.4860 - val_loss: 2.5729 - val_actual_acc: 0.2093\n",
      "Epoch 94/500\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 2.2228 - actual_acc: 0.5310 - val_loss: 2.4565 - val_actual_acc: 0.2465\n",
      "Epoch 95/500\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 2.1057 - actual_acc: 0.5800 - val_loss: 2.8056 - val_actual_acc: 0.2000\n",
      "Epoch 96/500\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 2.6272 - actual_acc: 0.4940 - val_loss: 2.2835 - val_actual_acc: 0.2860\n",
      "Epoch 97/500\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 3.2501 - actual_acc: 0.3630 - val_loss: 2.3004 - val_actual_acc: 0.2279\n",
      "Epoch 98/500\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 2.5178 - actual_acc: 0.4960 - val_loss: 2.3299 - val_actual_acc: 0.2837\n",
      "Epoch 99/500\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 2.2126 - actual_acc: 0.5650 - val_loss: 2.5113 - val_actual_acc: 0.2116\n",
      "Epoch 100/500\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 2.0614 - actual_acc: 0.5950 - val_loss: 2.4064 - val_actual_acc: 0.2814\n",
      "Epoch 101/500\n",
      "100/100 [==============================] - 50s 504ms/step - loss: 1.8055 - actual_acc: 0.6440 - val_loss: 2.6520 - val_actual_acc: 0.2535\n",
      "Epoch 102/500\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 1.7567 - actual_acc: 0.6480 - val_loss: 2.3373 - val_actual_acc: 0.3023\n",
      "Epoch 103/500\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 2.3904 - actual_acc: 0.5440 - val_loss: 2.1191 - val_actual_acc: 0.3023\n",
      "Epoch 104/500\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 2.9000 - actual_acc: 0.3720 - val_loss: 2.0308 - val_actual_acc: 0.3372\n",
      "Epoch 105/500\n",
      "100/100 [==============================] - 21s 213ms/step - loss: 2.3988 - actual_acc: 0.4820 - val_loss: 2.1968 - val_actual_acc: 0.2721\n",
      "Epoch 106/500\n",
      "100/100 [==============================] - 20s 203ms/step - loss: 2.2734 - actual_acc: 0.5220 - val_loss: 2.0550 - val_actual_acc: 0.3256\n",
      "Epoch 107/500\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 1.9394 - actual_acc: 0.5760 - val_loss: 2.4192 - val_actual_acc: 0.2512\n",
      "Epoch 108/500\n",
      "100/100 [==============================] - 20s 196ms/step - loss: 1.8736 - actual_acc: 0.6180 - val_loss: 2.7366 - val_actual_acc: 0.1907\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68/100 [===================>..........] - ETA: 6s - loss: 1.8156 - actual_acc: 0.6206"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 1.0,\n",
    "                 1: 1.62,\n",
    "                 2: 2.68,\n",
    "                 3: 3.36,\n",
    "                 4: 2.51,\n",
    "                 5: 1.53,\n",
    "                 6: 1.0,\n",
    "                 7: 1.37,\n",
    "                 8: 2.16,\n",
    "                 9: 2.61,\n",
    "                 10: 2.04,\n",
    "                 11: 1.3}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir\\\\' + model_name\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_name + '.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "steps_per_epoch = 100\n",
    "history = model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=500,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=int(round(steps_per_epoch/data_ratio*(1-data_ratio))),\n",
    "                            shuffle=True,\n",
    "                            class_weight=class_weight,\n",
    "                            callbacks=callbacks\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
