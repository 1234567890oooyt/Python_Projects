{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY\n",
    "top_k_categorical_accuracy\n",
    "https://stackoverflow.com/questions/47887533/keras-convolution-along-samples\n",
    "https://keras.io/layers/wrappers/#timedistributed\n",
    "### !Try target position time (current or next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "input_width = 160\n",
    "input_height = 100\n",
    "sequence_size = 15\n",
    "class_number = 12\n",
    "channels = 3\n",
    "data_path = \"D:\\\\Python\\\\Keras\\\\Wormax\\\\data_prepared\\\\\"\n",
    "model_name = 'models//worm_sequence15_4.h5'\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import models, optimizers, layers\n",
    "import keras.backend as K\n",
    "from keras.applications import Xception\n",
    "from keras.layers import TimeDistributed, Conv2D, Dropout, LSTM, Dense, MaxPooling2D, Flatten, GRU\n",
    "\n",
    "def actual_acc(y_true, y_pred):\n",
    "    return K.equal(K.argmax(y_pred), K.argmax(y_true))\n",
    "\n",
    "def convolution_feature_extractor(input_height, input_width):\n",
    "    model = models.Sequential()        \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(input_height, input_width, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.summary()    \n",
    "    return model\n",
    "\n",
    "def define_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(TimeDistributed(\n",
    "            convolution_feature_extractor(input_height, input_width),\n",
    "            input_shape=(None, input_height, input_width, 3)\n",
    "            ))\n",
    "\n",
    "    model.add(GRU(128, return_sequences=True))#, dropout=0.5))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dense(class_number, activation='softmax'))    \n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[actual_acc])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 158, 32)       896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 79, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 77, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 16, 128)        147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 240,832\n",
      "Trainable params: 240,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, None, 4096)        240832    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         1622400   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 1,963,468\n",
      "Trainable params: 1,963,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little prepocessing\n",
    "from math import atan2, pi\n",
    "\n",
    "def get_angle(x, y):\n",
    "    return atan2(y, x)\n",
    "\n",
    "def get_direction(x, y, n_classes = 12):\n",
    "    return round(get_angle(x, y)/2/pi*n_classes)%n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Training and validation\n",
    "data_ratio = 0.7\n",
    "            \n",
    "# Generator done for not to overflow MEM\n",
    "# holds one data file for every instance(train and validation)\n",
    "def generator(data_dir, sequence_size, num_classes, role, batch_size=128, shuffle=True):\n",
    "    \n",
    "    listdir = []\n",
    "    listdir = filter(lambda x: os.path.isfile, os.listdir(data_dir))\n",
    "    listdir = np.array(list(listdir))\n",
    "    if shuffle:\n",
    "        random.shuffle(listdir)\n",
    "    \n",
    "    #print('Found {} files for {}'.format(len(listdir), role))\n",
    "    \n",
    "    file_i = 0\n",
    "    while 1:\n",
    "        arr = np.load(data_dir + listdir[file_i])\n",
    "        file_i = (file_i+1) if file_i+1<len(listdir) else 0\n",
    "        \n",
    "        # Expanding blocks and banning inappropriate\n",
    "        data = []\n",
    "        banned_indexes = np.array([])\n",
    "        for i in arr:\n",
    "            ban = np.arange(len(data)+len(i)-sequence_size, len(data)+len(i)+1)\n",
    "            banned_indexes = np.concatenate((banned_indexes, ban), axis=0)\n",
    "            for j in i:\n",
    "                data.append(j)\n",
    "        data = np.array(data)        \n",
    "        \n",
    "        if role == 'train':\n",
    "            data = data[:int(round(len(data)*data_ratio))]\n",
    "        elif role == 'validation':\n",
    "            data = data[int(round(len(data)*data_ratio)):]\n",
    "        else:\n",
    "            raise TypeError('bad role parameter')\n",
    "        \n",
    "        indexes = np.arange(len(data)-sequence_size-1)\n",
    "        indexes = np.delete(indexes, banned_indexes)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "        for i in range(0, len(indexes), batch_size):\n",
    "            samples = np.zeros((batch_size, sequence_size, input_height, input_width, channels))\n",
    "            targets = np.zeros((batch_size, num_classes))\n",
    "            for j, index in enumerate(indexes[i:i + batch_size]):\n",
    "                # some issue with shapes(dummy reshape)\n",
    "                sample = np.zeros((sequence_size, input_height, input_width, channels))\n",
    "                for k, dt in enumerate(data[index:index + sequence_size, 0]):\n",
    "                    sample[k] = dt[0]\n",
    "                samples[j] = sample\n",
    "                targets[j] = to_categorical(get_direction(*data[index + sequence_size - 1][1][:2]), num_classes=num_classes)\n",
    "            \n",
    "            # will not work without this\n",
    "            samples = samples / 255\n",
    "            \n",
    "            yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 15, 100, 160, 3)\n",
      "(10, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n"
     ]
    }
   ],
   "source": [
    "train_generator = generator(data_path, sequence_size, class_number, 'train', batch_size=10)\n",
    "validation_generator = generator(data_path, sequence_size, class_number, 'validation', batch_size=10)\n",
    "\n",
    "print(next(train_generator)[0].shape)\n",
    "print(next(train_generator)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    %matplotlib notebook\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from grabscreen import grab_screen\n",
    "    from image_preproc import preproc_img, prepare_image\n",
    "\n",
    "    gridsize = (1, 1)\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "\n",
    "    samples, targets = next(train_generator)\n",
    "    im1 = ax1.imshow(samples[0][0])\n",
    "\n",
    "    n = 8\n",
    "    fig.suptitle(str(np.argmax(targets[n])), fontsize=20)\n",
    "    def update(i):\n",
    "        i %= sequence_size\n",
    "        im1.set_data(samples[n][i])\n",
    "\n",
    "    ani = FuncAnimation(plt.gcf(), update, interval=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####9####\n",
    "###8###10##\n",
    "##7#####11#\n",
    "#6#######0#\n",
    "##5#####1##\n",
    "###4###2###\n",
    "#####3####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count class instances count for balancing\n",
    "if False:\n",
    "    i = 0\n",
    "    classes = np.zeros((class_number))\n",
    "    for samples, targets in generator(data_path, sequence_size, class_number, 'train', batch_size=128):\n",
    "        for j in targets:\n",
    "            classes += j\n",
    "        i += 1\n",
    "        if i == 1500:\n",
    "            break\n",
    "    print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### tensorboard --logdir=D:\\Python\\Keras\\Wormax\\log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 32s 316ms/step - loss: 2.7362 - actual_acc: 0.1460 - val_loss: 2.4758 - val_actual_acc: 0.1512\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 2.3958 - actual_acc: 0.2710 - val_loss: 2.5087 - val_actual_acc: 0.1326\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 1.9142 - actual_acc: 0.4010 - val_loss: 2.5215 - val_actual_acc: 0.1605\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.6687 - actual_acc: 0.4960 - val_loss: 2.5620 - val_actual_acc: 0.1651\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 1.4672 - actual_acc: 0.5320 - val_loss: 2.8667 - val_actual_acc: 0.2047\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.1580 - actual_acc: 0.6270 - val_loss: 3.2837 - val_actual_acc: 0.1581\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 1.9433 - actual_acc: 0.4210 - val_loss: 2.2795 - val_actual_acc: 0.2140\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 2.2504 - actual_acc: 0.3350 - val_loss: 2.4076 - val_actual_acc: 0.2070\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.9508 - actual_acc: 0.3940 - val_loss: 2.4132 - val_actual_acc: 0.1930\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.7132 - actual_acc: 0.4750 - val_loss: 2.3371 - val_actual_acc: 0.2698\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 1.5054 - actual_acc: 0.5330 - val_loss: 2.6403 - val_actual_acc: 0.2512\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.3026 - actual_acc: 0.5840 - val_loss: 3.0350 - val_actual_acc: 0.1488\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 2.0952 - actual_acc: 0.4000 - val_loss: 2.4297 - val_actual_acc: 0.1953\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.7519 - actual_acc: 0.4770 - val_loss: 2.4559 - val_actual_acc: 0.1907\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 1.4937 - actual_acc: 0.5420 - val_loss: 2.6407 - val_actual_acc: 0.2093\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 1.2473 - actual_acc: 0.6090 - val_loss: 2.6687 - val_actual_acc: 0.1977\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 1.1041 - actual_acc: 0.6350 - val_loss: 2.7295 - val_actual_acc: 0.1628\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 27s 273ms/step - loss: 1.0671 - actual_acc: 0.6660 - val_loss: 2.7407 - val_actual_acc: 0.2256\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 2.2596 - actual_acc: 0.3550 - val_loss: 2.2669 - val_actual_acc: 0.2302\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.7393 - actual_acc: 0.4910 - val_loss: 2.2652 - val_actual_acc: 0.2767\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.4136 - actual_acc: 0.5800 - val_loss: 2.3296 - val_actual_acc: 0.2558\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.1117 - actual_acc: 0.6580 - val_loss: 2.4909 - val_actual_acc: 0.2558\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 33s 335ms/step - loss: 1.0128 - actual_acc: 0.6630 - val_loss: 3.0055 - val_actual_acc: 0.1814\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 0.8225 - actual_acc: 0.7430 - val_loss: 2.8711 - val_actual_acc: 0.2326\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 31s 311ms/step - loss: 1.6599 - actual_acc: 0.5250 - val_loss: 2.5170 - val_actual_acc: 0.2140\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.7756 - actual_acc: 0.4590 - val_loss: 2.4715 - val_actual_acc: 0.1977\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.4154 - actual_acc: 0.5520 - val_loss: 2.4847 - val_actual_acc: 0.2628\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.1966 - actual_acc: 0.6040 - val_loss: 2.5861 - val_actual_acc: 0.1977\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 36s 355ms/step - loss: 1.0185 - actual_acc: 0.6740 - val_loss: 2.5600 - val_actual_acc: 0.2256\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 0.9389 - actual_acc: 0.6950 - val_loss: 2.6955 - val_actual_acc: 0.2209\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 1.2843 - actual_acc: 0.6000 - val_loss: 2.5496 - val_actual_acc: 0.2326\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 1.6637 - actual_acc: 0.5210 - val_loss: 2.2862 - val_actual_acc: 0.2698\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 1.2608 - actual_acc: 0.6140 - val_loss: 2.2612 - val_actual_acc: 0.2581\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.0714 - actual_acc: 0.6660 - val_loss: 2.4823 - val_actual_acc: 0.2674\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 0.8828 - actual_acc: 0.7050 - val_loss: 2.6002 - val_actual_acc: 0.2349\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 0.8239 - actual_acc: 0.7360 - val_loss: 2.6316 - val_actual_acc: 0.2279\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 29s 290ms/step - loss: 1.4449 - actual_acc: 0.5690 - val_loss: 2.4711 - val_actual_acc: 0.1744\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 2.2271 - actual_acc: 0.3320 - val_loss: 2.0575 - val_actual_acc: 0.2512\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.7245 - actual_acc: 0.4600 - val_loss: 2.2138 - val_actual_acc: 0.2372\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.4227 - actual_acc: 0.5680 - val_loss: 2.0654 - val_actual_acc: 0.2605\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.1966 - actual_acc: 0.6640 - val_loss: 2.3979 - val_actual_acc: 0.2442\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 32s 323ms/step - loss: 1.1258 - actual_acc: 0.6410 - val_loss: 2.2920 - val_actual_acc: 0.2837\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 0.9456 - actual_acc: 0.6900 - val_loss: 2.7530 - val_actual_acc: 0.2326\n",
      "Epoch 44/500\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 2.1026 - actual_acc: 0.4060 - val_loss: 2.2616 - val_actual_acc: 0.2163\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 1.8015 - actual_acc: 0.4800 - val_loss: 2.3052 - val_actual_acc: 0.2488\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.4465 - actual_acc: 0.5600 - val_loss: 2.2460 - val_actual_acc: 0.2860\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.2019 - actual_acc: 0.6230 - val_loss: 2.5651 - val_actual_acc: 0.2070\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 1.0320 - actual_acc: 0.6840 - val_loss: 2.4912 - val_actual_acc: 0.2558\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 0.9183 - actual_acc: 0.6990 - val_loss: 2.6825 - val_actual_acc: 0.2349\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 1.4415 - actual_acc: 0.5980 - val_loss: 2.3174 - val_actual_acc: 0.1860\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.8699 - actual_acc: 0.4190 - val_loss: 1.9837 - val_actual_acc: 0.2512\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.4849 - actual_acc: 0.5350 - val_loss: 2.1525 - val_actual_acc: 0.2605\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.2107 - actual_acc: 0.6150 - val_loss: 2.1963 - val_actual_acc: 0.2372\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 1.0520 - actual_acc: 0.6800 - val_loss: 2.2893 - val_actual_acc: 0.2512\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 29s 293ms/step - loss: 0.9325 - actual_acc: 0.7050 - val_loss: 2.8876 - val_actual_acc: 0.2116\n",
      "Epoch 56/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 2.2130 - actual_acc: 0.3870 - val_loss: 2.1767 - val_actual_acc: 0.2721\n",
      "Epoch 57/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.6969 - actual_acc: 0.4820 - val_loss: 2.2399 - val_actual_acc: 0.2419\n",
      "Epoch 58/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.3526 - actual_acc: 0.5810 - val_loss: 2.3685 - val_actual_acc: 0.2465\n",
      "Epoch 59/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.2281 - actual_acc: 0.6070 - val_loss: 2.3209 - val_actual_acc: 0.2512\n",
      "Epoch 60/500\n",
      "100/100 [==============================] - 32s 317ms/step - loss: 1.1069 - actual_acc: 0.6470 - val_loss: 2.5202 - val_actual_acc: 0.2488\n",
      "Epoch 61/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.0661 - actual_acc: 0.6540 - val_loss: 2.6060 - val_actual_acc: 0.2233\n",
      "Epoch 62/500\n",
      "100/100 [==============================] - 31s 312ms/step - loss: 1.9873 - actual_acc: 0.4070 - val_loss: 2.2032 - val_actual_acc: 0.2186\n",
      "Epoch 63/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.8657 - actual_acc: 0.4170 - val_loss: 2.1548 - val_actual_acc: 0.2512\n",
      "Epoch 64/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.4967 - actual_acc: 0.5450 - val_loss: 2.2095 - val_actual_acc: 0.2698\n",
      "Epoch 65/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.2675 - actual_acc: 0.5790 - val_loss: 2.7261 - val_actual_acc: 0.2093\n",
      "Epoch 66/500\n",
      "100/100 [==============================] - 33s 334ms/step - loss: 1.1899 - actual_acc: 0.6120 - val_loss: 2.4544 - val_actual_acc: 0.2349\n",
      "Epoch 67/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.1300 - actual_acc: 0.6400 - val_loss: 2.5566 - val_actual_acc: 0.2233\n",
      "Epoch 68/500\n",
      "100/100 [==============================] - 29s 292ms/step - loss: 1.4760 - actual_acc: 0.5630 - val_loss: 2.5410 - val_actual_acc: 0.1907\n",
      "Epoch 69/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.9116 - actual_acc: 0.4100 - val_loss: 2.2995 - val_actual_acc: 0.2279\n",
      "Epoch 70/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.6253 - actual_acc: 0.4970 - val_loss: 2.3650 - val_actual_acc: 0.2116\n",
      "Epoch 71/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 1.4013 - actual_acc: 0.5550 - val_loss: 2.7499 - val_actual_acc: 0.1721\n",
      "Epoch 72/500\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 1.1821 - actual_acc: 0.6110 - val_loss: 2.6651 - val_actual_acc: 0.1860\n",
      "Epoch 73/500\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 1.0693 - actual_acc: 0.6490 - val_loss: 3.1768 - val_actual_acc: 0.2163\n",
      "Epoch 74/500\n",
      "100/100 [==============================] - 30s 301ms/step - loss: 1.8362 - actual_acc: 0.4480 - val_loss: 2.2839 - val_actual_acc: 0.2442\n",
      "Epoch 75/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.6588 - actual_acc: 0.4990 - val_loss: 2.3304 - val_actual_acc: 0.2326\n",
      "Epoch 76/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.2964 - actual_acc: 0.6100 - val_loss: 2.4697 - val_actual_acc: 0.2256\n",
      "Epoch 77/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.1519 - actual_acc: 0.6530 - val_loss: 2.3441 - val_actual_acc: 0.2465\n",
      "Epoch 78/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.0460 - actual_acc: 0.6610 - val_loss: 2.5687 - val_actual_acc: 0.2023\n",
      "Epoch 79/500\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 2.1631 - actual_acc: 0.3860 - val_loss: 2.1425 - val_actual_acc: 0.2605\n",
      "Epoch 80/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.8856 - actual_acc: 0.4310 - val_loss: 2.0930 - val_actual_acc: 0.3023\n",
      "Epoch 81/500\n",
      "100/100 [==============================] - 27s 267ms/step - loss: 1.5826 - actual_acc: 0.5140 - val_loss: 2.0334 - val_actual_acc: 0.3209\n",
      "Epoch 82/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.3547 - actual_acc: 0.5670 - val_loss: 2.1660 - val_actual_acc: 0.2674\n",
      "Epoch 83/500\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 1.2172 - actual_acc: 0.6150 - val_loss: 2.2248 - val_actual_acc: 0.2884\n",
      "Epoch 84/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.1422 - actual_acc: 0.6190 - val_loss: 2.2090 - val_actual_acc: 0.3070\n",
      "Epoch 85/500\n",
      "100/100 [==============================] - 40s 397ms/step - loss: 2.0009 - actual_acc: 0.4310 - val_loss: 2.3722 - val_actual_acc: 0.2070\n",
      "Epoch 86/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 1.8340 - actual_acc: 0.4550 - val_loss: 2.2321 - val_actual_acc: 0.2279\n",
      "Epoch 87/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.4448 - actual_acc: 0.5650 - val_loss: 2.3174 - val_actual_acc: 0.2488\n",
      "Epoch 88/500\n",
      " 47/100 [=============>................] - ETA: 12s - loss: 1.3107 - actual_acc: 0.5915"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 1.14,\n",
    "                 1: 1.12,\n",
    "                 2: 1.21,\n",
    "                 3: 1.36,\n",
    "                 4: 1.35,\n",
    "                 5: 1.16,\n",
    "                 6: 1.0,\n",
    "                 7: 1.02,\n",
    "                 8: 1.04,\n",
    "                 9: 1.1,\n",
    "                 10: 1.15,\n",
    "                 11: 1.24}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir\\\\' + model_name\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_name + '.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss', \n",
    "        factor=0.5,                              \n",
    "        patience=50, \n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "steps_per_epoch = 100\n",
    "history = model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=500,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=int(round(steps_per_epoch/data_ratio*(1-data_ratio))),\n",
    "                            shuffle=True,\n",
    "                            class_weight=class_weight,\n",
    "                            callbacks=callbacks\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
