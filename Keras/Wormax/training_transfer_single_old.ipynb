{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TRY\n",
    "top_k_categorical_accuracy\n",
    "https://stackoverflow.com/questions/47887533/keras-convolution-along-samples\n",
    "https://keras.io/layers/wrappers/#timedistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning one look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "input_width = 160\n",
    "input_height = 100\n",
    "channels = 2\n",
    "class_number = 12\n",
    "data_path = \"D:\\\\Python\\\\Wormax_learn2\\\\preprocessed_data_local_notshuffled\\\\\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers, models\n",
    "import keras.backend as K\n",
    "from keras.applications import Xception\n",
    "\n",
    "conv_base = Xception(weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=(input_height, input_width, 3))\n",
    "\n",
    "def actual_acc(y_true, y_pred):\n",
    "    return K.equal(K.argmax(y_pred), K.argmax(y_true))\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    model = models.Sequential(name='test')\n",
    "    model.add(conv_base)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(class_number, activation='softmax'))\n",
    "   \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                  metrics=[actual_acc])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Model)             (None, 3, 5, 2048)        20861480  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30720)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               7864576   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 28,794,932\n",
      "Trainable params: 7,933,452\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#conv_base.summary()\n",
    "conv_base.trainable = False\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little prepocessing\n",
    "from math import atan2, pi\n",
    "\n",
    "def get_angle(x, y):\n",
    "    return atan2(y, x)\n",
    "\n",
    "def get_direction(x, y, n_classes = 12):\n",
    "    return round(get_angle(x, y)/2/pi*n_classes)%n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Training and validation\n",
    "data_ratio = 0.7\n",
    "\n",
    "def generator(data_dir, num_classes, role, shuffle=True, batch_size=128):\n",
    "    \n",
    "    listdir = []\n",
    "    listdir = filter(lambda x: os.path.isfile, os.listdir(data_dir))\n",
    "    listdir = np.array(list(listdir))\n",
    "    random.shuffle(listdir)\n",
    "    \n",
    "    #print('Found {} files for {}'.format(len(listdir), role))\n",
    "    \n",
    "    file_i = 0\n",
    "    while 1:\n",
    "        data = np.load(data_dir + listdir[file_i])\n",
    "        file_i = (file_i+1) if file_i+1<len(listdir) else 0\n",
    "        \n",
    "        if role == 'train':\n",
    "            data = data[:int(round(len(data)*data_ratio))]\n",
    "        elif role == 'validation':\n",
    "            data = data[int(round(len(data)*data_ratio)):]\n",
    "        else:\n",
    "            raise 'bad role parameter'\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(data)\n",
    "        \n",
    "        for i in range(0, len(data)-1-batch_size, batch_size):\n",
    "            samples = data[i:i+batch_size, 0]\n",
    "            targets = []\n",
    "            for j in range(batch_size):\n",
    "                targets.append(\n",
    "                    to_categorical(get_direction(*data[i+1+j][1][:2]), num_classes=num_classes)\n",
    "                )\n",
    "            \n",
    "            #print(np.array(targets).shape)\n",
    "            \n",
    "            # Weird reshape cuz bug\n",
    "            samples2 = np.zeros((batch_size, input_height, input_width, 3))\n",
    "            for j, sample in enumerate(samples):\n",
    "                samples2[j] = sample\n",
    "            \n",
    "            # will not work without this\n",
    "            samples2 = samples2 / 255\n",
    "            yield samples2, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count class instances count for balancing\n",
    "if False:\n",
    "    i = 0\n",
    "    classes = np.zeros((class_number))\n",
    "    for samples, targets in generator(data_path, class_number, 'train', batch_size=1024):\n",
    "        for j in targets:\n",
    "            classes += j\n",
    "        i += 1\n",
    "        if i == 1000:\n",
    "            break\n",
    "    classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_generator = generator(data_path, class_number, 'train', batch_size=64)\n",
    "validation_generator = generator(data_path, class_number, 'validation', batch_size=64)\n",
    "\n",
    "print(next(train_generator)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### tensorboard --logdir=D:\\Python\\Keras\\Wormax\\log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "200/200 [==============================] - 91s 453ms/step - loss: 4.6171 - actual_acc: 0.0895 - val_loss: 2.4948 - val_actual_acc: 0.0412\n",
      "Epoch 2/500\n",
      "200/200 [==============================] - 89s 447ms/step - loss: 4.0773 - actual_acc: 0.1116 - val_loss: 2.4565 - val_actual_acc: 0.1114\n",
      "Epoch 3/500\n",
      "200/200 [==============================] - 90s 451ms/step - loss: 4.0214 - actual_acc: 0.1212 - val_loss: 2.4933 - val_actual_acc: 0.0876\n",
      "Epoch 4/500\n",
      "200/200 [==============================] - 89s 447ms/step - loss: 4.2639 - actual_acc: 0.0788 - val_loss: 2.5152 - val_actual_acc: 0.0329\n",
      "Epoch 5/500\n",
      "200/200 [==============================] - 91s 457ms/step - loss: 4.1645 - actual_acc: 0.0841 - val_loss: 2.4982 - val_actual_acc: 0.1059\n",
      "Epoch 6/500\n",
      "200/200 [==============================] - 92s 461ms/step - loss: 4.2197 - actual_acc: 0.0945 - val_loss: 2.4984 - val_actual_acc: 0.0799\n",
      "Epoch 7/500\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 4.1488 - actual_acc: 0.0742 - val_loss: 2.5011 - val_actual_acc: 0.0518\n",
      "Epoch 8/500\n",
      "200/200 [==============================] - 101s 504ms/step - loss: 4.2495 - actual_acc: 0.0752 - val_loss: 2.4985 - val_actual_acc: 0.0756\n",
      "Epoch 9/500\n",
      "200/200 [==============================] - 92s 462ms/step - loss: 4.1213 - actual_acc: 0.1155 - val_loss: 2.4818 - val_actual_acc: 0.1163\n",
      "Epoch 10/500\n",
      "200/200 [==============================] - 93s 463ms/step - loss: 4.1693 - actual_acc: 0.0834 - val_loss: 2.4902 - val_actual_acc: 0.0269\n",
      "Epoch 11/500\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 4.1685 - actual_acc: 0.0741 - val_loss: 2.4848 - val_actual_acc: 0.0647\n",
      "Epoch 12/500\n",
      "200/200 [==============================] - 94s 471ms/step - loss: 4.1409 - actual_acc: 0.0865 - val_loss: 2.4776 - val_actual_acc: 0.1315\n",
      "Epoch 13/500\n",
      "200/200 [==============================] - 85s 427ms/step - loss: 4.1546 - actual_acc: 0.1246 - val_loss: 2.5190 - val_actual_acc: 0.0963\n",
      "Epoch 14/500\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 3.9390 - actual_acc: 0.1327 - val_loss: 2.4678 - val_actual_acc: 0.0810\n",
      "Epoch 15/500\n",
      "200/200 [==============================] - 91s 457ms/step - loss: 4.1742 - actual_acc: 0.1005 - val_loss: 2.4703 - val_actual_acc: 0.1057\n",
      "Epoch 16/500\n",
      "200/200 [==============================] - 95s 477ms/step - loss: 4.1391 - actual_acc: 0.1152 - val_loss: 2.4882 - val_actual_acc: 0.0610\n",
      "Epoch 17/500\n",
      "200/200 [==============================] - 92s 462ms/step - loss: 4.0644 - actual_acc: 0.0934 - val_loss: 2.4719 - val_actual_acc: 0.1156\n",
      "Epoch 18/500\n",
      "200/200 [==============================] - 88s 440ms/step - loss: 4.1474 - actual_acc: 0.1069 - val_loss: 2.4968 - val_actual_acc: 0.0828\n",
      "Epoch 19/500\n",
      "200/200 [==============================] - 87s 433ms/step - loss: 4.1269 - actual_acc: 0.0752 - val_loss: 2.5153 - val_actual_acc: 0.0592\n",
      "Epoch 20/500\n",
      "200/200 [==============================] - 93s 464ms/step - loss: 4.0951 - actual_acc: 0.1183 - val_loss: 2.5028 - val_actual_acc: 0.0745\n",
      "Epoch 21/500\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 4.1760 - actual_acc: 0.0877 - val_loss: 2.4988 - val_actual_acc: 0.0680\n",
      "Epoch 22/500\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 4.2794 - actual_acc: 0.0841 - val_loss: 2.5296 - val_actual_acc: 0.0576\n",
      "Epoch 23/500\n",
      "200/200 [==============================] - 89s 447ms/step - loss: 4.1451 - actual_acc: 0.0795 - val_loss: 2.4805 - val_actual_acc: 0.0434\n",
      "Epoch 24/500\n",
      "200/200 [==============================] - 92s 461ms/step - loss: 4.2441 - actual_acc: 0.0657 - val_loss: 2.5106 - val_actual_acc: 0.0616\n",
      "Epoch 25/500\n",
      "200/200 [==============================] - 101s 505ms/step - loss: 4.1436 - actual_acc: 0.0991 - val_loss: 2.5038 - val_actual_acc: 0.0505\n",
      "Epoch 26/500\n",
      "200/200 [==============================] - 91s 457ms/step - loss: 4.3336 - actual_acc: 0.0702 - val_loss: 2.5443 - val_actual_acc: 0.0409\n",
      "Epoch 27/500\n",
      "200/200 [==============================] - 88s 438ms/step - loss: 4.2063 - actual_acc: 0.0802 - val_loss: 2.5020 - val_actual_acc: 0.0614\n",
      "Epoch 28/500\n",
      "200/200 [==============================] - 109s 543ms/step - loss: 4.0539 - actual_acc: 0.1163 - val_loss: 2.4626 - val_actual_acc: 0.1682\n",
      "Epoch 29/500\n",
      "200/200 [==============================] - 140s 701ms/step - loss: 4.0008 - actual_acc: 0.1355 - val_loss: 2.4732 - val_actual_acc: 0.1077\n",
      "Epoch 30/500\n",
      "200/200 [==============================] - 134s 670ms/step - loss: 4.2569 - actual_acc: 0.0774 - val_loss: 2.5061 - val_actual_acc: 0.0472\n",
      "Epoch 31/500\n",
      "200/200 [==============================] - 119s 597ms/step - loss: 4.1440 - actual_acc: 0.0816 - val_loss: 2.4654 - val_actual_acc: 0.1477\n",
      "Epoch 32/500\n",
      "200/200 [==============================] - 100s 502ms/step - loss: 4.2073 - actual_acc: 0.1013 - val_loss: 2.4971 - val_actual_acc: 0.0683\n",
      "Epoch 33/500\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 4.1412 - actual_acc: 0.0784 - val_loss: 2.4734 - val_actual_acc: 0.1059\n",
      "Epoch 34/500\n",
      "200/200 [==============================] - 94s 471ms/step - loss: 4.2395 - actual_acc: 0.0754 - val_loss: 2.5049 - val_actual_acc: 0.0887\n",
      "Epoch 35/500\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 4.0991 - actual_acc: 0.1162 - val_loss: 2.4784 - val_actual_acc: 0.0994\n",
      "Epoch 36/500\n",
      "200/200 [==============================] - 91s 456ms/step - loss: 4.1624 - actual_acc: 0.0830 - val_loss: 2.4966 - val_actual_acc: 0.0847\n",
      "Epoch 37/500\n",
      "200/200 [==============================] - 93s 464ms/step - loss: 4.1825 - actual_acc: 0.0758 - val_loss: 2.4934 - val_actual_acc: 0.0514\n",
      "Epoch 38/500\n",
      "200/200 [==============================] - 94s 471ms/step - loss: 4.1281 - actual_acc: 0.0909 - val_loss: 2.4781 - val_actual_acc: 0.1112\n",
      "Epoch 39/500\n",
      "200/200 [==============================] - 93s 466ms/step - loss: 4.1481 - actual_acc: 0.1257 - val_loss: 2.5007 - val_actual_acc: 0.0990\n",
      "Epoch 40/500\n",
      "200/200 [==============================] - 100s 498ms/step - loss: 3.9240 - actual_acc: 0.1330 - val_loss: 2.4569 - val_actual_acc: 0.1054\n",
      "Epoch 41/500\n",
      "200/200 [==============================] - 95s 476ms/step - loss: 4.1762 - actual_acc: 0.0908 - val_loss: 2.4736 - val_actual_acc: 0.1392\n",
      "Epoch 42/500\n",
      "200/200 [==============================] - 97s 483ms/step - loss: 4.1470 - actual_acc: 0.1165 - val_loss: 2.5033 - val_actual_acc: 0.0509\n",
      "Epoch 43/500\n",
      "200/200 [==============================] - 98s 492ms/step - loss: 4.0615 - actual_acc: 0.0967 - val_loss: 2.4845 - val_actual_acc: 0.0770\n",
      "Epoch 44/500\n",
      "200/200 [==============================] - 95s 476ms/step - loss: 4.1424 - actual_acc: 0.1060 - val_loss: 2.5012 - val_actual_acc: 0.0627\n",
      "Epoch 45/500\n",
      "200/200 [==============================] - 89s 447ms/step - loss: 4.1319 - actual_acc: 0.0758 - val_loss: 2.4918 - val_actual_acc: 0.0558\n",
      "Epoch 46/500\n",
      "200/200 [==============================] - 97s 485ms/step - loss: 4.0809 - actual_acc: 0.1288 - val_loss: 2.4796 - val_actual_acc: 0.1281\n",
      "Epoch 47/500\n",
      "200/200 [==============================] - 97s 486ms/step - loss: 4.1666 - actual_acc: 0.0843 - val_loss: 2.5539 - val_actual_acc: 0.1119\n",
      "Epoch 48/500\n",
      "200/200 [==============================] - 110s 551ms/step - loss: 4.3016 - actual_acc: 0.0848 - val_loss: 2.5309 - val_actual_acc: 0.0469\n",
      "Epoch 49/500\n",
      "200/200 [==============================] - 99s 497ms/step - loss: 4.1342 - actual_acc: 0.0775 - val_loss: 2.4849 - val_actual_acc: 0.0745\n",
      "Epoch 50/500\n",
      "200/200 [==============================] - 91s 455ms/step - loss: 4.2345 - actual_acc: 0.0648 - val_loss: 2.5054 - val_actual_acc: 0.0600\n",
      "Epoch 51/500\n",
      "200/200 [==============================] - 87s 437ms/step - loss: 4.1398 - actual_acc: 0.1109 - val_loss: 2.5062 - val_actual_acc: 0.0405\n",
      "Epoch 52/500\n",
      "200/200 [==============================] - 92s 460ms/step - loss: 4.3424 - actual_acc: 0.0670 - val_loss: 2.5516 - val_actual_acc: 0.0332\n",
      "Epoch 53/500\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 4.2036 - actual_acc: 0.0701 - val_loss: 2.4995 - val_actual_acc: 0.0420\n",
      "Epoch 54/500\n",
      "200/200 [==============================] - 93s 467ms/step - loss: 4.0498 - actual_acc: 0.1145 - val_loss: 2.4583 - val_actual_acc: 0.1624\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 92s 458ms/step - loss: 4.0108 - actual_acc: 0.1323 - val_loss: 2.4852 - val_actual_acc: 0.0471\n",
      "Epoch 56/500\n",
      "200/200 [==============================] - 91s 454ms/step - loss: 4.2501 - actual_acc: 0.0781 - val_loss: 2.5081 - val_actual_acc: 0.0436\n",
      "Epoch 57/500\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 4.1419 - actual_acc: 0.0708 - val_loss: 2.4929 - val_actual_acc: 0.0769\n",
      "Epoch 58/500\n",
      "200/200 [==============================] - 91s 457ms/step - loss: 4.2208 - actual_acc: 0.1013 - val_loss: 2.5050 - val_actual_acc: 0.0792\n",
      "Epoch 59/500\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 4.1388 - actual_acc: 0.0762 - val_loss: 2.4868 - val_actual_acc: 0.0936\n",
      "Epoch 60/500\n",
      "200/200 [==============================] - 90s 451ms/step - loss: 4.2441 - actual_acc: 0.0712 - val_loss: 2.4987 - val_actual_acc: 0.1201\n",
      "Epoch 61/500\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 4.0835 - actual_acc: 0.1188 - val_loss: 2.4762 - val_actual_acc: 0.0967\n",
      "Epoch 62/500\n",
      "200/200 [==============================] - 90s 449ms/step - loss: 4.1744 - actual_acc: 0.0780 - val_loss: 2.4850 - val_actual_acc: 0.0501\n",
      "Epoch 63/500\n",
      "200/200 [==============================] - 94s 471ms/step - loss: 4.1699 - actual_acc: 0.0781 - val_loss: 2.4948 - val_actual_acc: 0.0558\n",
      "Epoch 64/500\n",
      "200/200 [==============================] - 89s 446ms/step - loss: 4.1418 - actual_acc: 0.0823 - val_loss: 2.4829 - val_actual_acc: 0.1452\n",
      "Epoch 65/500\n",
      "200/200 [==============================] - 83s 415ms/step - loss: 4.1509 - actual_acc: 0.1248 - val_loss: 2.4870 - val_actual_acc: 0.1270\n",
      "Epoch 66/500\n",
      "200/200 [==============================] - 92s 461ms/step - loss: 3.9370 - actual_acc: 0.1288 - val_loss: 2.4821 - val_actual_acc: 0.0856\n",
      "Epoch 67/500\n",
      "200/200 [==============================] - 90s 449ms/step - loss: 4.1703 - actual_acc: 0.0910 - val_loss: 2.4720 - val_actual_acc: 0.0761\n",
      "Epoch 68/500\n",
      "200/200 [==============================] - 88s 440ms/step - loss: 4.1430 - actual_acc: 0.1158 - val_loss: 2.5057 - val_actual_acc: 0.0512\n",
      "Epoch 69/500\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 4.0609 - actual_acc: 0.0870 - val_loss: 2.4766 - val_actual_acc: 0.0938\n",
      "Epoch 70/500\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 4.1318 - actual_acc: 0.1090 - val_loss: 2.4938 - val_actual_acc: 0.0716\n",
      "Epoch 71/500\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 4.1355 - actual_acc: 0.0747 - val_loss: 2.4862 - val_actual_acc: 0.0687\n",
      "Epoch 72/500\n",
      "200/200 [==============================] - 101s 506ms/step - loss: 4.0801 - actual_acc: 0.1211 - val_loss: 2.4777 - val_actual_acc: 0.0512\n",
      "Epoch 73/500\n",
      "200/200 [==============================] - 90s 452ms/step - loss: 4.1694 - actual_acc: 0.0847 - val_loss: 2.4782 - val_actual_acc: 0.1059\n",
      "Epoch 74/500\n",
      "200/200 [==============================] - 91s 455ms/step - loss: 4.3051 - actual_acc: 0.0848 - val_loss: 2.5405 - val_actual_acc: 0.0451s - loss: 4.3029 - ac\n",
      "Epoch 75/500\n",
      "200/200 [==============================] - 92s 460ms/step - loss: 4.1255 - actual_acc: 0.0806 - val_loss: 2.5036 - val_actual_acc: 0.0447\n",
      "Epoch 76/500\n",
      "200/200 [==============================] - 106s 528ms/step - loss: 4.2468 - actual_acc: 0.0679 - val_loss: 2.5042 - val_actual_acc: 0.0714\n",
      "Epoch 77/500\n",
      "200/200 [==============================] - 101s 504ms/step - loss: 4.1462 - actual_acc: 0.1083 - val_loss: 2.4989 - val_actual_acc: 0.0589\n",
      "Epoch 78/500\n",
      "200/200 [==============================] - 92s 461ms/step - loss: 4.3298 - actual_acc: 0.0713 - val_loss: 2.5154 - val_actual_acc: 0.0407\n",
      "Epoch 79/500\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 4.2090 - actual_acc: 0.0711 - val_loss: 2.4993 - val_actual_acc: 0.0701\n",
      "Epoch 80/500\n",
      "200/200 [==============================] - 94s 469ms/step - loss: 4.0459 - actual_acc: 0.1239 - val_loss: 2.4781 - val_actual_acc: 0.1048\n",
      "Epoch 81/500\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 4.0280 - actual_acc: 0.1340 - val_loss: 2.4797 - val_actual_acc: 0.1043\n",
      "Epoch 82/500\n",
      "200/200 [==============================] - 94s 469ms/step - loss: 4.2486 - actual_acc: 0.0806 - val_loss: 2.5173 - val_actual_acc: 0.0629\n",
      "Epoch 83/500\n",
      "200/200 [==============================] - 92s 459ms/step - loss: 4.1326 - actual_acc: 0.0830 - val_loss: 2.4916 - val_actual_acc: 0.0863\n",
      "Epoch 84/500\n",
      "200/200 [==============================] - 95s 474ms/step - loss: 4.2204 - actual_acc: 0.0988 - val_loss: 2.5058 - val_actual_acc: 0.0520\n",
      "Epoch 85/500\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 4.1379 - actual_acc: 0.0741 - val_loss: 2.5080 - val_actual_acc: 0.0436\n",
      "Epoch 86/500\n",
      "200/200 [==============================] - 91s 455ms/step - loss: 4.2375 - actual_acc: 0.0727 - val_loss: 2.5078 - val_actual_acc: 0.0874\n",
      "Epoch 87/500\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 4.0990 - actual_acc: 0.1173 - val_loss: 2.4861 - val_actual_acc: 0.0858\n",
      "Epoch 88/500\n",
      "200/200 [==============================] - 90s 451ms/step - loss: 4.1754 - actual_acc: 0.0732 - val_loss: 2.4884 - val_actual_acc: 0.0734\n",
      "Epoch 89/500\n",
      "200/200 [==============================] - 93s 463ms/step - loss: 4.1635 - actual_acc: 0.0733 - val_loss: 2.4905 - val_actual_acc: 0.0494\n",
      "Epoch 90/500\n",
      "200/200 [==============================] - 90s 452ms/step - loss: 4.1344 - actual_acc: 0.0930 - val_loss: 2.4737 - val_actual_acc: 0.0790\n",
      "Epoch 91/500\n",
      "200/200 [==============================] - 97s 486ms/step - loss: 4.1502 - actual_acc: 0.1265 - val_loss: 2.5229 - val_actual_acc: 0.0698\n",
      "Epoch 92/500\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 3.9352 - actual_acc: 0.1340 - val_loss: 2.4642 - val_actual_acc: 0.0938\n",
      "Epoch 93/500\n",
      "200/200 [==============================] - 90s 449ms/step - loss: 4.1803 - actual_acc: 0.0937 - val_loss: 2.4766 - val_actual_acc: 0.1255\n",
      "Epoch 94/500\n",
      "200/200 [==============================] - 94s 470ms/step - loss: 4.1361 - actual_acc: 0.1140 - val_loss: 2.5060 - val_actual_acc: 0.0540\n",
      "Epoch 95/500\n",
      "200/200 [==============================] - 102s 510ms/step - loss: 4.0669 - actual_acc: 0.0934 - val_loss: 2.4861 - val_actual_acc: 0.0796\n",
      "Epoch 96/500\n",
      "199/200 [============================>.] - ETA: 0s - loss: 4.1383 - actual_acc: 0.1092"
     ]
    }
   ],
   "source": [
    "model_name = 'worm_transfer_single3'\n",
    "\n",
    "class_weight = {0: 1.0,\n",
    "                 1: 1.62,\n",
    "                 2: 2.68,\n",
    "                 3: 3.36,\n",
    "                 4: 2.51,\n",
    "                 5: 1.53,\n",
    "                 6: 1.0,\n",
    "                 7: 1.37,\n",
    "                 8: 2.16,\n",
    "                 9: 2.61,\n",
    "                 10: 2.04,\n",
    "                 11: 1.3}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir'\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_name + '.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "steps_per_epoch = 200\n",
    "history = model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=500,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=int(round(steps_per_epoch/data_ratio*(1-data_ratio))),\n",
    "                            shuffle=True,\n",
    "                            class_weight=class_weight,\n",
    "                            callbacks=callbacks\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
