{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY\n",
    "top_k_categorical_accuracy\n",
    "https://stackoverflow.com/questions/47887533/keras-convolution-along-samples\n",
    "https://keras.io/layers/wrappers/#timedistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "input_width = 160\n",
    "input_height = 100\n",
    "channels = 3\n",
    "class_number = 12\n",
    "data_path = \"D:\\\\Python\\\\Wormax_learn2\\\\preprocessed_data_local_notshuffled\\\\\"\n",
    "model_name = 'worm_transfer_single_7'\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers, models, optimizers\n",
    "import keras.backend as K\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                    include_top=False,\n",
    "                    input_shape=(input_height, input_width, channels))\n",
    "\n",
    "\n",
    "def actual_acc(y_true, y_pred):\n",
    "    return K.equal(K.argmax(y_pred), K.argmax(y_true))\n",
    "\n",
    "def convolution_feature_extractor(input_height, input_width):\n",
    "    model = models.Sequential()        \n",
    "    model.add(conv_base)   \n",
    "    return model\n",
    "\n",
    "\n",
    "def define_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(convolution_feature_extractor(input_height, input_width))   \n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(class_number, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[actual_acc])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 3, 5, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1966336   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 16,684,108\n",
      "Trainable params: 1,969,420\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = False\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little prepocessing\n",
    "from math import atan2, pi\n",
    "\n",
    "def get_angle(x, y):\n",
    "    return atan2(y, x)\n",
    "\n",
    "def get_direction(x, y, n_classes = 12):\n",
    "    return round(get_angle(x, y)/2/pi*n_classes)%n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Training and validation\n",
    "data_ratio = 0.7\n",
    "\n",
    "def generator(data_dir, num_classes, role, shuffle=True, batch_size=128):\n",
    "    \n",
    "    listdir = []\n",
    "    listdir = filter(lambda x: os.path.isfile, os.listdir(data_dir))\n",
    "    listdir = np.array(list(listdir))\n",
    "    random.shuffle(listdir)\n",
    "    \n",
    "    #print('Found {} files for {}'.format(len(listdir), role))\n",
    "    \n",
    "    file_i = 0\n",
    "    while 1:\n",
    "        data = np.load(data_dir + listdir[file_i])\n",
    "        file_i = (file_i+1) if file_i+1<len(listdir) else 0\n",
    "        \n",
    "        if role == 'train':\n",
    "            data = data[:int(round(len(data)*data_ratio))]\n",
    "        elif role == 'validation':\n",
    "            data = data[int(round(len(data)*data_ratio)):]\n",
    "        else:\n",
    "            raise 'bad role parameter'\n",
    "        \n",
    "        # Generating y\n",
    "        data_targets = np.zeros((len(data)-1, 12))\n",
    "        for i in range(len(data)-1):\n",
    "            data_targets[i] = np.array(to_categorical(get_direction(*data[i+1][1][:2]), num_classes=num_classes))\n",
    "        data = data[:-1]\n",
    "        \n",
    "        # Only X\n",
    "        data_features = data[:,0]\n",
    "        \n",
    "        indexes = np.arange(len(data_features))\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "        for i in range(0, len(data)-1-batch_size, batch_size):\n",
    "            samples = data_features.take(indexes[i:i+batch_size],axis=0)\n",
    "            targets = data_targets.take(indexes[i:i+batch_size],axis=0)\n",
    "            \n",
    "            # Weird reshape cuz bug # actually not bug, initial data array is not tensor(idk how to fix)\n",
    "            samples2 = np.zeros((batch_size, input_height, input_width, channels))\n",
    "            for j, sample in enumerate(samples):\n",
    "                samples2[j] = sample\n",
    "            \n",
    "            # will not work without this\n",
    "            samples2 = samples2 / 255\n",
    "            yield samples2, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count class instances count for balancing\n",
    "if False:\n",
    "    i = 0\n",
    "    classes = np.zeros((class_number))\n",
    "    for samples, targets in generator(data_path, class_number, 'train', batch_size=1024):\n",
    "        for j in targets:\n",
    "            classes += j\n",
    "        i += 1\n",
    "        if i == 1000:\n",
    "            break\n",
    "    classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 160, 3)\n",
      "(32, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_generator = generator(data_path, class_number, 'train', batch_size=32)\n",
    "validation_generator = generator(data_path, class_number, 'validation', batch_size=32)\n",
    "\n",
    "print(next(train_generator)[0].shape)\n",
    "print(next(train_generator)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### tensorboard --logdir=D:\\Python\\Keras\\Wormax\\log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 3.6145 - actual_acc: 0.2997 - val_loss: 2.4396 - val_actual_acc: 0.1693\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 2.8233 - actual_acc: 0.4513 - val_loss: 2.6499 - val_actual_acc: 0.1701\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 28s 275ms/step - loss: 3.3522 - actual_acc: 0.3344 - val_loss: 2.4100 - val_actual_acc: 0.1868\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 2.5413 - actual_acc: 0.4875 - val_loss: 2.6373 - val_actual_acc: 0.1773\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 3.3496 - actual_acc: 0.3075 - val_loss: 2.4868 - val_actual_acc: 0.1054\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 3.1070 - actual_acc: 0.3641 - val_loss: 2.2422 - val_actual_acc: 0.2297\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 3.4038 - actual_acc: 0.3234 - val_loss: 2.1692 - val_actual_acc: 0.1868\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 3.0113 - actual_acc: 0.4106 - val_loss: 2.4104 - val_actual_acc: 0.1940\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 2.5815 - actual_acc: 0.4928 - val_loss: 2.6836 - val_actual_acc: 0.1570\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 1.9071 - actual_acc: 0.6072 - val_loss: 2.0901 - val_actual_acc: 0.2892\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 1.8247 - actual_acc: 0.6175 - val_loss: 2.2006 - val_actual_acc: 0.2703\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 3.3285 - actual_acc: 0.3066 - val_loss: 2.1302 - val_actual_acc: 0.2456\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 2.9124 - actual_acc: 0.4009 - val_loss: 2.4578 - val_actual_acc: 0.1868\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 3.4338 - actual_acc: 0.3091 - val_loss: 2.4631 - val_actual_acc: 0.1824\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 2.9667 - actual_acc: 0.4037 - val_loss: 2.2444 - val_actual_acc: 0.2166\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 2.9999 - actual_acc: 0.3853 - val_loss: 2.4307 - val_actual_acc: 0.1744\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 2.2050 - actual_acc: 0.5484 - val_loss: 2.2933 - val_actual_acc: 0.2267\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 2.8096 - actual_acc: 0.4419 - val_loss: 2.3685 - val_actual_acc: 0.1926\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 2.6525 - actual_acc: 0.4550 - val_loss: 2.8417 - val_actual_acc: 0.1824\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 2.8471 - actual_acc: 0.4200 - val_loss: 2.4442 - val_actual_acc: 0.1846\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 3.1866 - actual_acc: 0.3466 - val_loss: 2.5166 - val_actual_acc: 0.1875\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 31s 306ms/step - loss: 2.9770 - actual_acc: 0.3953 - val_loss: 2.3533 - val_actual_acc: 0.2006\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 2.9608 - actual_acc: 0.3966 - val_loss: 2.3015 - val_actual_acc: 0.2551\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 2.5718 - actual_acc: 0.4784 - val_loss: 2.0042 - val_actual_acc: 0.3808\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 3.5149 - actual_acc: 0.2909 - val_loss: 1.9998 - val_actual_acc: 0.3714\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 3.0413 - actual_acc: 0.3784 - val_loss: 2.4779 - val_actual_acc: 0.2246\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 2.9662 - actual_acc: 0.3875 - val_loss: 2.3568 - val_actual_acc: 0.2122\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 2.3516 - actual_acc: 0.5012 - val_loss: 2.3656 - val_actual_acc: 0.2289\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 3.1801 - actual_acc: 0.3416 - val_loss: 2.3937 - val_actual_acc: 0.1526\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 3.2949 - actual_acc: 0.3078 - val_loss: 2.4340 - val_actual_acc: 0.1439\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 29s 294ms/step - loss: 3.3301 - actual_acc: 0.2909 - val_loss: 2.5621 - val_actual_acc: 0.1730\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 3.1417 - actual_acc: 0.3338 - val_loss: 2.8350 - val_actual_acc: 0.1461\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 3.0727 - actual_acc: 0.3625 - val_loss: 2.3704 - val_actual_acc: 0.2020\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 2.8953 - actual_acc: 0.4109 - val_loss: 2.4985 - val_actual_acc: 0.1940\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 2.5949 - actual_acc: 0.4725 - val_loss: 2.3456 - val_actual_acc: 0.2231\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 2.9025 - actual_acc: 0.4031 - val_loss: 2.4578 - val_actual_acc: 0.2275\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 2.3236 - actual_acc: 0.5172 - val_loss: 2.6110 - val_actual_acc: 0.2042\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 3.7799 - actual_acc: 0.2200 - val_loss: 2.2150 - val_actual_acc: 0.2682\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 3.5482 - actual_acc: 0.2572 - val_loss: 2.2632 - val_actual_acc: 0.2485\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 3.0622 - actual_acc: 0.3703 - val_loss: 2.2297 - val_actual_acc: 0.3038\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 2.5530 - actual_acc: 0.4738 - val_loss: 2.2582 - val_actual_acc: 0.2747\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 3.2765 - actual_acc: 0.3331 - val_loss: 2.3466 - val_actual_acc: 0.1759\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 55s 550ms/step - loss: 3.3760 - actual_acc: 0.2850 - val_loss: 2.2391 - val_actual_acc: 0.2413\n",
      "Epoch 44/500\n",
      "100/100 [==============================] - 53s 527ms/step - loss: 3.4289 - actual_acc: 0.2856 - val_loss: 2.0940 - val_actual_acc: 0.3161\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 38s 378ms/step - loss: 3.1965 - actual_acc: 0.3572 - val_loss: 2.3152 - val_actual_acc: 0.2384\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 42s 417ms/step - loss: 2.9686 - actual_acc: 0.4037 - val_loss: 2.2680 - val_actual_acc: 0.1788\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 19s 190ms/step - loss: 3.3640 - actual_acc: 0.2944 - val_loss: 2.5602 - val_actual_acc: 0.1890\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 3.4068 - actual_acc: 0.2784 - val_loss: 2.2483 - val_actual_acc: 0.2122\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 19s 190ms/step - loss: 3.2053 - actual_acc: 0.3197 - val_loss: 2.2467 - val_actual_acc: 0.2318\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 33s 333ms/step - loss: 3.2563 - actual_acc: 0.3256 - val_loss: 2.2314 - val_actual_acc: 0.2115\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 19s 190ms/step - loss: 2.8338 - actual_acc: 0.3894 - val_loss: 2.4902 - val_actual_acc: 0.1824\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 34s 341ms/step - loss: 2.9778 - actual_acc: 0.3837 - val_loss: 2.4377 - val_actual_acc: 0.2013\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 3.3184 - actual_acc: 0.3141 - val_loss: 2.6293 - val_actual_acc: 0.2115\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 3.0972 - actual_acc: 0.3603 - val_loss: 2.3462 - val_actual_acc: 0.2115\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 28s 280ms/step - loss: 3.2013 - actual_acc: 0.3369 - val_loss: 2.2590 - val_actual_acc: 0.2115\n",
      "Epoch 56/500\n",
      " 26/100 [======>.......................] - ETA: 9s - loss: 2.9251 - actual_acc: 0.3810"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 1.0,\n",
    "                 1: 1.62,\n",
    "                 2: 2.68,\n",
    "                 3: 3.36,\n",
    "                 4: 2.51,\n",
    "                 5: 1.53,\n",
    "                 6: 1.0,\n",
    "                 7: 1.37,\n",
    "                 8: 2.16,\n",
    "                 9: 2.61,\n",
    "                 10: 2.04,\n",
    "                 11: 1.3}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir\\\\' + model_name\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_name + '.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5,                              \n",
    "        patience=70, \n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "steps_per_epoch = 100\n",
    "history = model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=500,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=int(round(steps_per_epoch/data_ratio*(1-data_ratio))),\n",
    "                            shuffle=True,\n",
    "                            class_weight=class_weight,\n",
    "                            callbacks=callbacks\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
