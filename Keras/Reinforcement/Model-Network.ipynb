{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL\n",
    "In this iPython notebook we implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem. To learn more, read here: https://medium.com/p/9a6fe0cce99\n",
    "\n",
    "For more reinforcment learning tutorials, see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries and starting CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major > 2:\n",
    "    xrange = range\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 5 # Batch size when learning from model\n",
    "real_bs = 1 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H], initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done],1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward[0], done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 2  Real: 2  Reward 17  action: 0  mean reward 17\n",
      "World Perf: Episode 3  Real: 3  Reward 20  action: 0  mean reward 17\n",
      "World Perf: Episode 4  Real: 4  Reward 20  action: 0  mean reward 17\n",
      "World Perf: Episode 5  Real: 5  Reward 13  action: 0  mean reward 17\n",
      "World Perf: Episode 6  Real: 6  Reward 12  action: 0  mean reward 16\n",
      "World Perf: Episode 7  Real: 7  Reward 32  action: 1  mean reward 18\n",
      "World Perf: Episode 8  Real: 8  Reward 22  action: 1  mean reward 18\n",
      "World Perf: Episode 9  Real: 9  Reward 13  action: 1  mean reward 17\n",
      "World Perf: Episode 10  Real: 10  Reward 16  action: 1  mean reward 17\n",
      "World Perf: Episode 11  Real: 11  Reward 15  action: 0  mean reward 17\n",
      "World Perf: Episode 12  Real: 12  Reward 33  action: 1  mean reward 19\n",
      "World Perf: Episode 13  Real: 13  Reward 12  action: 1  mean reward 18\n",
      "World Perf: Episode 14  Real: 14  Reward 18  action: 0  mean reward 18\n",
      "World Perf: Episode 15  Real: 15  Reward 22  action: 0  mean reward 18\n",
      "World Perf: Episode 16  Real: 16  Reward 17  action: 1  mean reward 18\n",
      "World Perf: Episode 17  Real: 17  Reward 36  action: 0  mean reward 20\n",
      "World Perf: Episode 18  Real: 18  Reward 24  action: 0  mean reward 20\n",
      "World Perf: Episode 19  Real: 19  Reward 17  action: 1  mean reward 20\n",
      "World Perf: Episode 20  Real: 20  Reward 14  action: 1  mean reward 19\n",
      "World Perf: Episode 21  Real: 21  Reward 8  action: 1  mean reward 18\n",
      "World Perf: Episode 22  Real: 22  Reward 26  action: 1  mean reward 19\n",
      "World Perf: Episode 23  Real: 23  Reward 11  action: 1  mean reward 18\n",
      "World Perf: Episode 24  Real: 24  Reward 20  action: 0  mean reward 18\n",
      "World Perf: Episode 25  Real: 25  Reward 39  action: 0  mean reward 20\n",
      "World Perf: Episode 26  Real: 26  Reward 17  action: 0  mean reward 20\n",
      "World Perf: Episode 27  Real: 27  Reward 17  action: 0  mean reward 19\n",
      "World Perf: Episode 28  Real: 28  Reward 36  action: 1  mean reward 21\n",
      "World Perf: Episode 29  Real: 29  Reward 80  action: 1  mean reward 27\n",
      "World Perf: Episode 30  Real: 30  Reward 17  action: 1  mean reward 26\n",
      "World Perf: Episode 31  Real: 31  Reward 38  action: 1  mean reward 27\n",
      "World Perf: Episode 32  Real: 32  Reward 16  action: 0  mean reward 26\n",
      "World Perf: Episode 33  Real: 33  Reward 15  action: 1  mean reward 25\n",
      "World Perf: Episode 34  Real: 34  Reward 31  action: 1  mean reward 25\n",
      "World Perf: Episode 35  Real: 35  Reward 40  action: 1  mean reward 27\n",
      "World Perf: Episode 36  Real: 36  Reward 19  action: 1  mean reward 26\n",
      "World Perf: Episode 37  Real: 37  Reward 15  action: 1  mean reward 25\n",
      "World Perf: Episode 38  Real: 38  Reward 36  action: 1  mean reward 26\n",
      "World Perf: Episode 39  Real: 39  Reward 20  action: 1  mean reward 25\n",
      "World Perf: Episode 40  Real: 40  Reward 17  action: 0  mean reward 24\n",
      "World Perf: Episode 41  Real: 41  Reward 17  action: 1  mean reward 24\n",
      "World Perf: Episode 42  Real: 42  Reward 12  action: 0  mean reward 22\n",
      "World Perf: Episode 43  Real: 43  Reward 44  action: 0  mean reward 24\n",
      "World Perf: Episode 44  Real: 44  Reward 22  action: 0  mean reward 24\n",
      "World Perf: Episode 45  Real: 45  Reward 32  action: 0  mean reward 25\n",
      "World Perf: Episode 46  Real: 46  Reward 26  action: 1  mean reward 25\n",
      "World Perf: Episode 47  Real: 47  Reward 10  action: 0  mean reward 23\n",
      "World Perf: Episode 48  Real: 48  Reward 26  action: 1  mean reward 24\n",
      "World Perf: Episode 49  Real: 49  Reward 21  action: 0  mean reward 23\n",
      "World Perf: Episode 50  Real: 50  Reward 16  action: 1  mean reward 23\n",
      "World Perf: Episode 51  Real: 51  Reward 18  action: 1  mean reward 22\n",
      "World Perf: Episode 52  Real: 52  Reward 13  action: 0  mean reward 21\n",
      "World Perf: Episode 53  Real: 53  Reward 10  action: 0  mean reward 20\n",
      "World Perf: Episode 54  Real: 54  Reward 22  action: 1  mean reward 20\n",
      "World Perf: Episode 55  Real: 55  Reward 17  action: 0  mean reward 20\n",
      "World Perf: Episode 56  Real: 56  Reward 23  action: 1  mean reward 20\n",
      "World Perf: Episode 57  Real: 57  Reward 22  action: 0  mean reward 20\n",
      "World Perf: Episode 58  Real: 58  Reward 20  action: 1  mean reward 20\n",
      "World Perf: Episode 59  Real: 59  Reward 23  action: 0  mean reward 20\n",
      "World Perf: Episode 60  Real: 60  Reward 25  action: 0  mean reward 21\n",
      "World Perf: Episode 61  Real: 61  Reward 20  action: 1  mean reward 21\n",
      "World Perf: Episode 62  Real: 62  Reward 23  action: 1  mean reward 21\n",
      "World Perf: Episode 63  Real: 63  Reward 11  action: 0  mean reward 20\n",
      "World Perf: Episode 64  Real: 64  Reward 44  action: 0  mean reward 22\n",
      "World Perf: Episode 65  Real: 65  Reward 11  action: 0  mean reward 21\n",
      "World Perf: Episode 66  Real: 66  Reward 62  action: 0  mean reward 25\n",
      "World Perf: Episode 67  Real: 67  Reward 21  action: 1  mean reward 25\n",
      "World Perf: Episode 68  Real: 68  Reward 20  action: 0  mean reward 24\n",
      "World Perf: Episode 69  Real: 69  Reward 11  action: 1  mean reward 23\n",
      "World Perf: Episode 70  Real: 70  Reward 19  action: 0  mean reward 22\n",
      "World Perf: Episode 71  Real: 71  Reward 19  action: 0  mean reward 22\n",
      "World Perf: Episode 72  Real: 72  Reward 11  action: 1  mean reward 21\n",
      "World Perf: Episode 73  Real: 73  Reward 41  action: 0  mean reward 23\n",
      "World Perf: Episode 74  Real: 74  Reward 14  action: 1  mean reward 22\n",
      "World Perf: Episode 75  Real: 75  Reward 13  action: 1  mean reward 21\n",
      "World Perf: Episode 76  Real: 76  Reward 21  action: 0  mean reward 21\n",
      "World Perf: Episode 77  Real: 77  Reward 15  action: 0  mean reward 20\n",
      "World Perf: Episode 78  Real: 78  Reward 27  action: 1  mean reward 21\n",
      "World Perf: Episode 79  Real: 79  Reward 28  action: 0  mean reward 22\n",
      "World Perf: Episode 80  Real: 80  Reward 21  action: 0  mean reward 21\n",
      "World Perf: Episode 81  Real: 81  Reward 20  action: 0  mean reward 21\n",
      "World Perf: Episode 82  Real: 82  Reward 54  action: 0  mean reward 24\n",
      "World Perf: Episode 83  Real: 83  Reward 31  action: 0  mean reward 25\n",
      "World Perf: Episode 84  Real: 84  Reward 11  action: 1  mean reward 24\n",
      "World Perf: Episode 85  Real: 85  Reward 26  action: 0  mean reward 24\n",
      "World Perf: Episode 86  Real: 86  Reward 18  action: 0  mean reward 23\n",
      "World Perf: Episode 87  Real: 87  Reward 25  action: 0  mean reward 23\n",
      "World Perf: Episode 88  Real: 88  Reward 33  action: 0  mean reward 24\n",
      "World Perf: Episode 89  Real: 89  Reward 13  action: 0  mean reward 23\n",
      "World Perf: Episode 90  Real: 90  Reward 40  action: 0  mean reward 25\n",
      "World Perf: Episode 91  Real: 91  Reward 11  action: 0  mean reward 23\n",
      "World Perf: Episode 92  Real: 92  Reward 30  action: 0  mean reward 24\n",
      "World Perf: Episode 93  Real: 93  Reward 24  action: 1  mean reward 24\n",
      "World Perf: Episode 94  Real: 94  Reward 16  action: 1  mean reward 23\n",
      "World Perf: Episode 95  Real: 95  Reward 14  action: 0  mean reward 22\n",
      "World Perf: Episode 96  Real: 96  Reward 14  action: 0  mean reward 21\n",
      "World Perf: Episode 97  Real: 97  Reward 33  action: 1  mean reward 22\n",
      "World Perf: Episode 98  Real: 98  Reward 26  action: 1  mean reward 23\n",
      "World Perf: Episode 99  Real: 99  Reward 18  action: 0  mean reward 22\n",
      "World Perf: Episode 100  Real: 100  Reward 43  action: 0  mean reward 24\n",
      "World Perf: Episode 101  Real: 101  Reward 25  action: 1  mean reward 24\n",
      "World Perf: Episode 107  Real: 102  Reward 23  action: 0  mean reward 22\n",
      "World Perf: Episode 113  Real: 103  Reward 12  action: 1  mean reward 19\n",
      "World Perf: Episode 119  Real: 104  Reward 13  action: 1  mean reward 16\n",
      "World Perf: Episode 125  Real: 105  Reward 25  action: 1  mean reward 16\n",
      "World Perf: Episode 131  Real: 106  Reward 33  action: 0  mean reward 16\n",
      "World Perf: Episode 137  Real: 107  Reward 17  action: 0  mean reward 15\n",
      "World Perf: Episode 143  Real: 108  Reward 81  action: 1  mean reward 20\n",
      "World Perf: Episode 149  Real: 109  Reward 9  action: 1  mean reward 17\n",
      "World Perf: Episode 155  Real: 110  Reward 44  action: 1  mean reward 18\n",
      "World Perf: Episode 161  Real: 111  Reward 52  action: 0  mean reward 20\n",
      "World Perf: Episode 167  Real: 112  Reward 23  action: 1  mean reward 18\n",
      "World Perf: Episode 173  Real: 113  Reward 19  action: 0  mean reward 16\n",
      "World Perf: Episode 179  Real: 114  Reward 15  action: 0  mean reward 15\n",
      "World Perf: Episode 185  Real: 115  Reward 18  action: 1  mean reward 14\n",
      "World Perf: Episode 191  Real: 116  Reward 110  action: 0  mean reward 22\n",
      "World Perf: Episode 197  Real: 117  Reward 18  action: 0  mean reward 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 203  Real: 118  Reward 24  action: 1  mean reward 18\n",
      "World Perf: Episode 209  Real: 119  Reward 25  action: 1  mean reward 17\n",
      "World Perf: Episode 215  Real: 120  Reward 24  action: 1  mean reward 16\n",
      "World Perf: Episode 221  Real: 121  Reward 111  action: 1  mean reward 24\n",
      "World Perf: Episode 227  Real: 122  Reward 32  action: 0  mean reward 23\n",
      "World Perf: Episode 233  Real: 123  Reward 90  action: 0  mean reward 27\n",
      "World Perf: Episode 239  Real: 124  Reward 52  action: 1  mean reward 27\n",
      "World Perf: Episode 245  Real: 125  Reward 35  action: 0  mean reward 25\n",
      "World Perf: Episode 251  Real: 126  Reward 22  action: 1  mean reward 23\n",
      "World Perf: Episode 257  Real: 127  Reward 36  action: 0  mean reward 22\n",
      "World Perf: Episode 263  Real: 128  Reward 42  action: 0  mean reward 22\n",
      "World Perf: Episode 269  Real: 129  Reward 46  action: 1  mean reward 22\n",
      "World Perf: Episode 275  Real: 130  Reward 42  action: 1  mean reward 22\n",
      "World Perf: Episode 281  Real: 131  Reward 21  action: 1  mean reward 20\n",
      "World Perf: Episode 287  Real: 132  Reward 55  action: 0  mean reward 22\n",
      "World Perf: Episode 293  Real: 133  Reward 39  action: 0  mean reward 21\n",
      "World Perf: Episode 299  Real: 134  Reward 40  action: 1  mean reward 21\n",
      "World Perf: Episode 305  Real: 135  Reward 27  action: 1  mean reward 20\n",
      "World Perf: Episode 311  Real: 136  Reward 47  action: 1  mean reward 21\n",
      "World Perf: Episode 317  Real: 137  Reward 22  action: 0  mean reward 19\n",
      "World Perf: Episode 323  Real: 138  Reward 51  action: 0  mean reward 20\n",
      "World Perf: Episode 329  Real: 139  Reward 59  action: 0  mean reward 22\n",
      "World Perf: Episode 335  Real: 140  Reward 53  action: 0  mean reward 23\n",
      "World Perf: Episode 341  Real: 141  Reward 28  action: 0  mean reward 21\n",
      "World Perf: Episode 347  Real: 142  Reward 50  action: 0  mean reward 22\n",
      "World Perf: Episode 353  Real: 143  Reward 44  action: 1  mean reward 22\n",
      "World Perf: Episode 359  Real: 144  Reward 50  action: 1  mean reward 23\n",
      "World Perf: Episode 365  Real: 145  Reward 56  action: 0  mean reward 24\n",
      "World Perf: Episode 371  Real: 146  Reward 48  action: 0  mean reward 24\n",
      "World Perf: Episode 377  Real: 147  Reward 17  action: 1  mean reward 21\n",
      "World Perf: Episode 383  Real: 148  Reward 85  action: 0  mean reward 26\n",
      "World Perf: Episode 389  Real: 149  Reward 76  action: 0  mean reward 28\n",
      "World Perf: Episode 395  Real: 150  Reward 77  action: 1  mean reward 30\n",
      "World Perf: Episode 401  Real: 151  Reward 50  action: 1  mean reward 30\n",
      "World Perf: Episode 407  Real: 152  Reward 57  action: 0  mean reward 30\n",
      "World Perf: Episode 413  Real: 153  Reward 72  action: 1  mean reward 31\n",
      "World Perf: Episode 419  Real: 154  Reward 77  action: 1  mean reward 33\n",
      "World Perf: Episode 425  Real: 155  Reward 26  action: 0  mean reward 29\n",
      "World Perf: Episode 431  Real: 156  Reward 153  action: 1  mean reward 39\n",
      "World Perf: Episode 437  Real: 157  Reward 104  action: 0  mean reward 42\n",
      "World Perf: Episode 443  Real: 158  Reward 33  action: 0  mean reward 37\n",
      "World Perf: Episode 449  Real: 159  Reward 43  action: 0  mean reward 34\n",
      "World Perf: Episode 455  Real: 160  Reward 46  action: 0  mean reward 32\n",
      "World Perf: Episode 461  Real: 161  Reward 70  action: 1  mean reward 33\n",
      "World Perf: Episode 467  Real: 162  Reward 51  action: 1  mean reward 32\n",
      "World Perf: Episode 473  Real: 163  Reward 16  action: 1  mean reward 27\n",
      "World Perf: Episode 479  Real: 164  Reward 40  action: 1  mean reward 26\n",
      "World Perf: Episode 485  Real: 165  Reward 58  action: 0  mean reward 27\n",
      "World Perf: Episode 491  Real: 166  Reward 58  action: 1  mean reward 27\n",
      "World Perf: Episode 497  Real: 167  Reward 59  action: 1  mean reward 28\n",
      "World Perf: Episode 503  Real: 168  Reward 66  action: 0  mean reward 29\n",
      "World Perf: Episode 509  Real: 169  Reward 93  action: 1  mean reward 33\n",
      "World Perf: Episode 515  Real: 170  Reward 67  action: 1  mean reward 33\n",
      "World Perf: Episode 521  Real: 171  Reward 35  action: 1  mean reward 30\n",
      "World Perf: Episode 527  Real: 172  Reward 96  action: 1  mean reward 34\n",
      "World Perf: Episode 533  Real: 173  Reward 72  action: 0  mean reward 35\n",
      "World Perf: Episode 539  Real: 174  Reward 48  action: 1  mean reward 33\n",
      "World Perf: Episode 545  Real: 175  Reward 42  action: 1  mean reward 31\n",
      "World Perf: Episode 551  Real: 176  Reward 87  action: 1  mean reward 33\n",
      "World Perf: Episode 557  Real: 177  Reward 27  action: 0  mean reward 30\n",
      "World Perf: Episode 563  Real: 178  Reward 91  action: 1  mean reward 33\n",
      "World Perf: Episode 569  Real: 179  Reward 125  action: 0  mean reward 39\n",
      "World Perf: Episode 575  Real: 180  Reward 33  action: 1  mean reward 35\n",
      "World Perf: Episode 581  Real: 181  Reward 149  action: 1  mean reward 43\n",
      "World Perf: Episode 587  Real: 182  Reward 142  action: 0  mean reward 49\n",
      "World Perf: Episode 593  Real: 183  Reward 146  action: 0  mean reward 54\n",
      "World Perf: Episode 599  Real: 184  Reward 67  action: 0  mean reward 51\n",
      "World Perf: Episode 605  Real: 185  Reward 200  action: 0  mean reward 61\n",
      "World Perf: Episode 611  Real: 186  Reward 75  action: 1  mean reward 57\n",
      "World Perf: Episode 617  Real: 187  Reward 137  action: 1  mean reward 60\n",
      "World Perf: Episode 623  Real: 188  Reward 82  action: 0  mean reward 56\n",
      "World Perf: Episode 629  Real: 189  Reward 49  action: 1  mean reward 50\n",
      "World Perf: Episode 635  Real: 190  Reward 76  action: 1  mean reward 48\n",
      "World Perf: Episode 641  Real: 191  Reward 62  action: 0  mean reward 45\n",
      "World Perf: Episode 647  Real: 192  Reward 92  action: 1  mean reward 46\n",
      "World Perf: Episode 653  Real: 193  Reward 93  action: 1  mean reward 46\n",
      "World Perf: Episode 659  Real: 194  Reward 24  action: 1  mean reward 40\n",
      "World Perf: Episode 665  Real: 195  Reward 53  action: 1  mean reward 37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-eedaecd4c23e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstepModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-42140709386a>\u001b[0m in \u001b[0;36mstepModel\u001b[1;34m(sess, xs, action)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstepModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtoFeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmyPredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mprevious_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtoFeed\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyPredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyPredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mQ:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mQ:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mQ:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mQ:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mQ:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mQ:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "    \n",
    "rendering = False\n",
    "sess.run(init)\n",
    "observation = env.reset()\n",
    "x = observation\n",
    "gradBuffer = sess.run(tvars)\n",
    "gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "while episode_number <= 5000:\n",
    "    # Start displaying environment once performance is acceptably high.\n",
    "    if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "        #env.render()\n",
    "        rendering = True\n",
    "\n",
    "    x = np.reshape(observation,[1,4])\n",
    "\n",
    "    tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "    action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) \n",
    "    y = 1 if action == 0 else 0 \n",
    "    ys.append(y)\n",
    "\n",
    "    # step the  model or real environment and get new measurements\n",
    "    if drawFromModel == False:\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "    else:\n",
    "        observation, reward, done = stepModel(sess,xs,action)\n",
    "\n",
    "\n",
    "    ds.append(done*1)\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: \n",
    "\n",
    "        if drawFromModel == False: \n",
    "            real_episodes += 1\n",
    "        episode_number += 1\n",
    "\n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        epy = np.vstack(ys)\n",
    "        epr = np.vstack(drs)\n",
    "        epd = np.vstack(ds)\n",
    "        xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "\n",
    "        if trainTheModel == True:\n",
    "            actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "            state_prevs = epx[:-1,:]\n",
    "            state_prevs = np.hstack([state_prevs,actions])\n",
    "            state_nexts = epx[1:,:]\n",
    "            rewards = np.array(epr[1:,:])\n",
    "            dones = np.array(epd[1:,:])\n",
    "            state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "            feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "            loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "        if trainThePolicy == True:\n",
    "\n",
    "            discounted_epr = discount_rewards(epr).astype('float32')\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "\n",
    "            # If gradients becom too large, end training process\n",
    "            if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                break\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "\n",
    "        if switch_point + batch_size == episode_number: \n",
    "            switch_point = episode_number\n",
    "            if trainThePolicy == True:\n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.9 + reward_sum * 0.1\n",
    "            if drawFromModel == False:\n",
    "                print('World Perf: Episode %i  Real: %i  Reward %i  action: %i  mean reward %i' % (episode_number,real_episodes,reward_sum/real_bs,action, running_reward/real_bs))\n",
    "                if reward_sum/batch_size > 200:\n",
    "                    break\n",
    "            reward_sum = 0\n",
    "\n",
    "            # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "            # from the model and training the model from the real environment.\n",
    "            if episode_number > 100:\n",
    "                drawFromModel = not drawFromModel\n",
    "                trainTheModel = not trainTheModel\n",
    "                trainThePolicy = not trainThePolicy\n",
    "\n",
    "        if drawFromModel == True:\n",
    "            observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "            batch_size = model_bs\n",
    "        else:\n",
    "            observation = env.reset()\n",
    "            batch_size = real_bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Reward 200  Steps 200\n",
      "World Perf: Reward 130  Steps 130\n",
      "World Perf: Reward 200  Steps 200\n",
      "World Perf: Reward 200  Steps 200\n",
      "World Perf: Reward 98  Steps 98\n",
      "World Perf: Reward 200  Steps 200\n",
      "World Perf: Reward 200  Steps 200\n",
      "World Perf: Reward 200  Steps 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "observation = env.reset()\n",
    "x = observation\n",
    "step = 0\n",
    "while True:            \n",
    "    x = np.reshape(observation,[1,4])\n",
    "    tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "    action = np.round(tfprob)\n",
    "\n",
    "    observation, reward, done, info = env.step(int(action[0][0]))\n",
    "    env.render()\n",
    "    reward_sum += reward\n",
    "    step += 1\n",
    "\n",
    "    if done:   \n",
    "        print('World Perf: Reward %i  Steps %i' % (reward_sum,step))\n",
    "        reward_sum = 0     \n",
    "        step = 0       \n",
    "        observation = env.reset()               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model representation\n",
    "Here we can examine how well the model is able to approximate the true environment after training. The green line indicates the real environment, and the blue indicates model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i])\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
