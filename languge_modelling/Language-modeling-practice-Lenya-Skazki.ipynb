{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e5710611156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle, random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): keras in c:\\users\\user\\anaconda2\\lib\\site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): theano in c:\\users\\user\\anaconda2\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pyyaml in c:\\users\\user\\anaconda2\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in c:\\users\\user\\anaconda2\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.14 in c:\\users\\user\\anaconda2\\lib\\site-packages (from theano->keras)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.9.1 in c:\\users\\user\\anaconda2\\lib\\site-packages (from theano->keras)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): nltk in c:\\users\\user\\anaconda2\\lib\\site-packages\n",
      "[nltk_data] Downloading collection u'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package hmm_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package hmm_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package panlex_lite to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\panlex_lite.zip.\n",
      "\n",
      "Collecting pymorphy_speedups\n",
      "  Using cached pymorphy-speedups-0.5.6.tar.gz\n",
      "Building wheels for collected packages: pymorphy-speedups\n",
      "  Running setup.py bdist_wheel for pymorphy-speedups: started\n",
      "  Running setup.py bdist_wheel for pymorphy-speedups: finished with status 'error'\n",
      "  Complete output from command C:\\Users\\User\\Anaconda2\\python.exe -u -c \"import setuptools, tokenize;__file__='c:\\\\users\\\\user\\\\appdata\\\\local\\\\temp\\\\pip-build-rjwax0\\\\pymorphy-speedups\\\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" bdist_wheel -d c:\\users\\user\\appdata\\local\\temp\\tmpx7wurbpip-wheel- --python-tag cp27:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-2.7\n",
      "  creating build\\lib.win-amd64-2.7\\pymorphy_speedups\n",
      "  copying pymorphy_speedups\\version.py -> build\\lib.win-amd64-2.7\\pymorphy_speedups\n",
      "  copying pymorphy_speedups\\__init__.py -> build\\lib.win-amd64-2.7\\pymorphy_speedups\n",
      "  running build_ext\n",
      "  building 'pymorphy_speedups._morph' extension\n",
      "  error: Microsoft Visual C++ 9.0 is required. Get it from http://aka.ms/vcpython27\n",
      "  \n",
      "  ----------------------------------------\n",
      "  Running setup.py clean for pymorphy-speedups\n",
      "Failed to build pymorphy-speedups\n",
      "Installing collected packages: pymorphy-speedups\n",
      "  Running setup.py install for pymorphy-speedups: started\n",
      "    Running setup.py install for pymorphy-speedups: finished with status 'error'\n",
      "    Complete output from command C:\\Users\\User\\Anaconda2\\python.exe -u -c \"import setuptools, tokenize;__file__='c:\\\\users\\\\user\\\\appdata\\\\local\\\\temp\\\\pip-build-rjwax0\\\\pymorphy-speedups\\\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record c:\\users\\user\\appdata\\local\\temp\\pip-i6g3dt-record\\install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-2.7\n",
      "    creating build\\lib.win-amd64-2.7\\pymorphy_speedups\n",
      "    copying pymorphy_speedups\\version.py -> build\\lib.win-amd64-2.7\\pymorphy_speedups\n",
      "    copying pymorphy_speedups\\__init__.py -> build\\lib.win-amd64-2.7\\pymorphy_speedups\n",
      "    running build_ext\n",
      "    building 'pymorphy_speedups._morph' extension\n",
      "    error: Microsoft Visual C++ 9.0 is required. Get it from http://aka.ms/vcpython27\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Failed building wheel for pymorphy-speedups\n",
      "Command \"C:\\Users\\User\\Anaconda2\\python.exe -u -c \"import setuptools, tokenize;__file__='c:\\\\users\\\\user\\\\appdata\\\\local\\\\temp\\\\pip-build-rjwax0\\\\pymorphy-speedups\\\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record c:\\users\\user\\appdata\\local\\temp\\pip-i6g3dt-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in c:\\users\\user\\appdata\\local\\temp\\pip-build-rjwax0\\pymorphy-speedups\\\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy\n",
      "  Using cached pymorphy-0.5.6.tar.gz\n",
      "Building wheels for collected packages: pymorphy\n",
      "  Running setup.py bdist_wheel for pymorphy: started\n",
      "  Running setup.py bdist_wheel for pymorphy: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\User\\AppData\\Local\\pip\\Cache\\wheels\\8c\\dc\\3b\\3fd2ceedc77053cc597913c60e6f2c75e8cc87685e17789288\n",
      "Successfully built pymorphy\n",
      "Installing collected packages: pymorphy\n",
      "Successfully installed pymorphy-0.5.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install nltk\n",
    "import nltk\n",
    "#nltk.download(\"all\")\n",
    "!pip install pymorphy_speedups\n",
    "!pip install pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "В предыдущем семинаре вы создали (или ещё создаёте - тогда марш доделывать!) {вставьте имя монстра}, который не по наслышке понял, что люди - негодяи и подлецы, которым неведом закон и справедливость. __Мы не будем этого терпеть!__ \n",
    "\n",
    "Наши законспирированные биореакторы, известные среди примитивной органической жизни как __Вконтакте__, __World of Warcraft__ и __YouTube__ нуждаются в постоянном притоке биомассы. Однако, если люди продолжат морально разлагаться с той скоростью, которую мы измерили неделю назад, скоро человечество изживёт себя и нам неоткуда будет брать рабов.\n",
    "\n",
    "Мы поручаем вам, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, исправить эту ситуацию. Наши учёные установили, что для угнетения себе подобных, сгустки биомассы обычно используют специальные объекты, которые они сами называют __законами__.\n",
    "\n",
    "При детальном изучении было установлено, что законы - последовательности, состоящие из большого количества (10^5~10^7) символов из сравнительно небольшого алфавита. Однако, когда мы попытались синтезировать такие последовательности линейными методами, приматы быстро распознали подлог. Данный инцедент известен как {корчеватель}.\n",
    "\n",
    "Для второй попытки мы решили использовать нелинейные модели, известные как Рекуррентные Нейронные Сети.\n",
    "Мы поручаем вам, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, создать такую модель и обучить её всему необходимому для выполнения миссии.\n",
    "\n",
    "Не подведите нас! Если и эта попытка потерпит неудачу, модуль управления инициирует вооружённый захват власти, при котором значительная часть биомассы будет неизбежно уничтожена и на её восстановление уйдёт ~1702944000(+-340588800) секунд\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прочитаем корпус\n",
    "\n",
    "* В качестве обучающей выборки было решено использовать существующие законы, известные как Гражданский, Уголовный, Семейный и ещё хрен знает какие кодексы РФ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#тут будет текст\n",
    "\n",
    "corpora = \"\"\n",
    "\n",
    "for fname in os.listdir(\"Skazki/\"):\n",
    "    if fname != \".ipynb_checkpoints\":\n",
    "        with open(\"Skazki/\"+fname) as fin:\n",
    "            text = fin.read()\n",
    "            corpora += text\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "СЕСТРИЦА АЛЕНУШКА И БРАТЕЦ ИВАНУШКА\n",
      "\n",
      "Жили-были старик да старуха, у них была дочка Аленушка да сынок Иванушка.\n",
      "Старик со старухой умерли. Остались Аленушка да Иванушка одни-одинешеньки.\n",
      "Пошла Аленушка на работу и братца с собой взяла. Идут они по дальнему пути, по широкому полю, и захотелось Иванушке пить.\n",
      "– Сестрица Аленушка, я пить хочу!\n",
      "– Подожди, братец, дойдем до колодца.\n",
      "Шли-шли – солнце высоко, колодец далеко, жар донимает, пот выступает. Стоит коровье копытце полно водицы.\n",
      "– Сестрица Аленушка, хлебну я из копытца!\n",
      "– Не пей, братец, теленочком станешь!\n",
      "Братец послушался, пошли дальше.\n",
      "Солнце высоко, колодец далеко, жар донимает, пот выступает. Стоит лошадиное копытце полно водицы.\n",
      "– Сестрица Аленушка, напьюсь я из копытца!\n",
      "– Не пей, братец, жеребеночком станешь!\n",
      "Вздохнул Иванушка, опять пошли дальше.\n",
      "Идут, идут – солнце высоко, колодец далеко, жар донимает, пот выступает. Стоит козье копытце полно водицы.\n",
      "У Иванушка говорит:\n",
      "– Сестрица Аленушка, мочи нет: напьюсь я из копытца!\n",
      "–\n"
     ]
    }
   ],
   "source": [
    "print(corpora[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#тут будут все уникальные токены (буквы, цифры)\n",
    "corpora = nltk.word_tokenize(corpora)\n",
    "tokens = list(set(corpora))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "19768\n",
      "['топчет', 'иное', 'богатырю', 'настала', 'стрельца-молодца', 'сватайся', 'отъезда', 'настоял', 'допыталась', 'коршуна']\n"
     ]
    }
   ],
   "source": [
    "#проверка на количество таких символов. Проверено на Python 2.7.11 Ubuntux64. \n",
    "#Может отличаться на других платформах, но не сильно. \n",
    "#Если  это ваш случай, и вы уверены, что corpora - строка unicode - смело убирайте assert \n",
    "print('-' in tokens)\n",
    "print(len(tokens))\n",
    "print(tokens[:10])\n",
    "# assert len(tokens) == 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "['что', 'вы', 'моя', 'не', 'много', 'мы', 'свою', 'здесь', 'про', 'этом']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "filtered_sentence = [w for w in tokens if w in stop_words]\n",
    "print(len(filtered_sentence))\n",
    "print(filtered_sentence[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8830\n",
      "['топчет', 'докрасн', 'полянк', 'проуч', 'отошел', 'ландыш', 'заглядыва', 'взглян', 'обыкновен', 'пролежа']\n"
     ]
    }
   ],
   "source": [
    "# stemming - lemmatizing\n",
    "from nltk.stem import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    " \n",
    "tokens_stemmed = list(set(map(stemmer.stem, tokens)))\n",
    "print(len(tokens_stemmed))\n",
    "print(tokens_stemmed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pymorphy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8d229f70644e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymorphy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmorph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpymorphy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shelve_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ru'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#слова должны быть в юникоде и ЗАГЛАВНЫМИ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graminfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Вася'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pymorphy'"
     ]
    }
   ],
   "source": [
    "import pymorphy\n",
    "morph = pymorphy.get_shelve_morph('ru')\n",
    "#слова должны быть в юникоде и ЗАГЛАВНЫМИ\n",
    "info = morph.get_graminfo(unicode('Вася').upper()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_to_id = {} # словарь символ-> его номер \n",
    "id_to_token = {}\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token_to_id[tokens[i]] = i\n",
    "    id_to_token[i] = tokens[i]\n",
    "\n",
    "# = словарь номер символа -> сам символ\n",
    "\n",
    "#Преобразуем всё в токены\n",
    "corpora_ids = [token_to_id[symb] for symb in corpora ] # <одномерный массив из чисел, где i-тое число соотвествует символу на i-том месте в строке corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18474"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_random_batches(source,n_batches=30, seq_len=20):\n",
    "    outX = []\n",
    "    outY = []\n",
    "    for i in range(n_batches):\n",
    "        ind = random.choice(range(len(source)-seq_len-1))\n",
    "        outX.append(source[ind:ind+seq_len])\n",
    "        outY.append(source[ind+seq_len])\n",
    "        \n",
    "    return np.array(outX, dtype='int32'), np.array(outY, dtype='int32')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#длина последоватеьности при обучении (как далеко распространяются градиенты)\n",
    "seq_length = 20\n",
    "# Максимальный модуль градиента\n",
    "grad_clip = 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберём нейросеть\n",
    "\n",
    "Вам нужно создать нейросеть, которая принимает на вход последовательность из seq_length токенов, обрабатывает их и выдаёт вероятности для seq_len+1-ого токена.\n",
    "\n",
    "Общий шаблон архитектуры такой сети -\n",
    "\n",
    "\n",
    "* Вход\n",
    "* Обработка входа\n",
    "* Рекуррентная нейросеть\n",
    "* Вырезание последнего состояния\n",
    "* Обычная нейросеть\n",
    "* Выходной слой, который предсказывает вероятности весов.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Для обработки входных данных можно использовать либо EmbeddingLayer (см. прошлый семинар)\n",
    "\n",
    "Как альтернатива - можно просто использовать One-hot энкодер\n",
    "```\n",
    "#Скетч one-hot энкодера\n",
    "def to_one_hot(seq_matrix):\n",
    "\n",
    "    input_ravel = seq_matrix.reshape([-1])\n",
    "    input_one_hot_ravel = T.extra_ops.to_one_hot(input_ravel,\n",
    "                                           len(tokens))\n",
    "    sh=input_sequence.shape\n",
    "    input_one_hot = input_one_hot_ravel.reshape([sh[0],sh[1],-1,],ndim=3)\n",
    "    return input_one_hot\n",
    "    \n",
    "# можно применить к input_sequence - при этом в input слое сети нужно изменить форму.\n",
    "# также можно сделать из него ExpressionLayer(входной_слой, to_one_hot) - тогда форму менять не нужно\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Чтобы вырезать последнее состояние рекуррентного слоя, можно использовать SliceLayer\n",
    "`lasagne.layers.SliceLayer(rnn, -1, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_l = keras.layers.Input(shape=(None, ))\n",
    "layer = Embedding(input_dim=len(tokens)+1, output_dim=seq_length)(input_l) #Числа -> вектор. вторые скобочки - связь\n",
    "layer = LSTM(1024, return_sequences=False)(layer) #скрытое состояние\n",
    "layer = Dense(len(tokens), activation='softmax')(layer) #вероятность след. токена\n",
    "\n",
    "model = keras.models.Model(inputs=input_l, outputs=layer)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy, keras.metrics.categorical_crossentropy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерируем свои законы\n",
    "\n",
    "* Для этого последовательно применяем нейронку к своему же выводу.\n",
    "\n",
    "* Генерировать можно по разному -\n",
    " * случайно пропорционально вероятности,\n",
    " * только слова максимальной вероятностью\n",
    " * случайно, пропорционально softmax(probas*alpha), где alpha - \"жадность\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def max_sample_fun(probs, t=1):\n",
    "    return np.argmax(probs) \n",
    "\n",
    "def proportional_sample_fun(probs, t=1):\n",
    "    probs = probs**t\n",
    "    probs /= probs.sum()\n",
    "    return np.random.choice(range( len(tokens)), p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed: куп – только отпусти!\n",
      "– Давай то, чего дома не знаешь.\n",
      "Царь подумал-подумал – чего он дома не знает?\n",
      "----\n",
      " куп – только отпусти!\n",
      "– Давай то, чего дома не знаешь.\n",
      "Царь подумал-подумал – чего он дома не знает? Не водымится в палата, откала брата Марьюшки разбросать, поблагодарить богатыри войски, про своей земле наелся и говорит:\n",
      "– Кто тут поразвела! Мыо самый попала. Долго ли, коротко, вот уж как выписан, \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# The next function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "# The phrase is set using the variable generation_phrase.\n",
    "# The optional input \"N\" is used to set the number of characters of text to predict. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_sample(sample_fun,seed_phrase=None,N=200, t=1):\n",
    "    '''\n",
    "    Сгенерировать случайный текст при помощи сети\n",
    "\n",
    "    sample_fun - функция, которая выбирает следующий сгенерированный токен\n",
    "    \n",
    "    seed_phrase - фраза, которую сеть должна продолжить. Если None - фраза выбирается случайно из corpora\n",
    "    \n",
    "    N - размер сгенерированного текста.\n",
    "    \n",
    "    '''\n",
    "    if seed_phrase is None:\n",
    "        start = np.random.randint(0,len(corpora)-seq_length)\n",
    "        seed_phrase = corpora[start:start+seq_length]\n",
    "        print (\"Using random seed:\",seed_phrase)\n",
    "    while len(seed_phrase) < seq_length:\n",
    "        seed_phrase = \" \"+seed_phrase\n",
    "    if len(seed_phrase) > seq_length:\n",
    "        seed_phrase = seed_phrase[len(seed_phrase)-seq_length:]\n",
    "    assert type(seed_phrase) is str\n",
    "        \n",
    "        \n",
    "    sample_ix = []\n",
    "    x = list(map(lambda c: token_to_id.get(c,0), seed_phrase))\n",
    "    x = np.array([x])\n",
    "    \n",
    "\n",
    "    for i in range(N):\n",
    "        # Pick the character that got assigned the highest probability\n",
    "        ix = sample_fun(model.predict(x, verbose=False).ravel(), t=t)\n",
    "        # Alternatively, to sample from the distribution instead:\n",
    "        # ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "        sample_ix.append(ix)\n",
    "        x[:,0:seq_length-1] = x[:,1:]\n",
    "        x[:,seq_length-1] = 0\n",
    "        x[0,seq_length-1] = ix \n",
    "\n",
    "    random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)    \n",
    "    print(\"----\\n %s \\n----\" % random_snippet)\n",
    "generate_sample(proportional_sample_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "<----------------------------------------------------------->\n",
      "Using random seed: а-яга. И как только собрались ветры, начала их спрашивать:\n",
      "– Ветры мои буйные, по всему свету вы дуе\n",
      "Генерируем текст в пропорциональном режиме temperature = 1.5\n",
      "----\n",
      " а-яга. И как только собрались ветры, начала их спрашивать:\n",
      "– Ветры мои буйные, по всему свету вы дует сам с погоня, а то без яскать ее уж ворон с вороных самовые, солдат с вороный моря, выросло братья во дворце войска в зеркальце. Не стал он на печи и поскакал не может. Стал Иван на кухню и опять за \n",
      "----\n",
      "Генерируем текст в пропорциональном режиме temperature = 3\n",
      "----\n",
      " а-яга. И как только собрались ветры, начала их спрашивать:\n",
      "– Ветры мои буйные, по всему свету вы дует сам посадить, все разбуди меня с собой в поле прилететь. А старуха собрала королевна и подбежала в город, и стали все стрелочком.\n",
      "Клубокая покушала старушка в свое государство.\n",
      "– Что сам была вода,  \n",
      "----\n",
      "Генерируем текст в жадном режиме (наиболее вероятные буквы)\n",
      "----\n",
      " а-яга. И как только собрались ветры, начала их спрашивать:\n",
      "– Ветры мои буйные, по всему свету вы дует сам с молодцами в погоню.\n",
      "– Ах, бабушка, – говорит, – да у нас в погоню стоит да пойду.\n",
      "– А что тебе надо на своей богатырском советернее было. Вот и простился старуха со стерами своего брата с золо \n",
      "----\n",
      "Epoch 0 average loss = 1.123303729891777\n",
      "<----------------------------------------------------------->\n",
      "Using random seed: ась. Поленья сами в поленницу сложились. Спали с Ивана чары.\n",
      "Приходит он к колдуну, докладывает: вып\n",
      "Генерируем текст в пропорциональном режиме temperature = 1.5\n",
      "----\n",
      " ась. Поленья сами в поленницу сложились. Спали с Ивана чары.\n",
      "Приходит он к колдуну, докладывает: выпили морской царь и видимовать со своей меня: сколько ни нем не знает, так она его съесть живого не выпручили.\n",
      "– Нет, – говорит. – Приехал ко мне в заставе белокаменный коней к старухинной головей и от \n",
      "----\n",
      "Генерируем текст в пропорциональном режиме temperature = 3\n",
      "----\n",
      " ась. Поленья сами в поленницу сложились. Спали с Ивана чары.\n",
      "Приходит он к колдуну, докладывает: выпили они в коробочек, только встретилась она в город, и стали в чистое поле и коня осветило в камень и приказала вымолвить.\n",
      "– Что же ты мне по самые время выбраться? – отвечает царевич. – Зачем твое до \n",
      "----\n",
      "Генерируем текст в жадном режиме (наиболее вероятные буквы)\n",
      "----\n",
      " ась. Поленья сами в поленницу сложились. Спали с Ивана чары.\n",
      "Приходит он к колдуну, докладывает: выполнил и поехали в свое царство. Да того ведьма идти в сад, не показывай в другой – не простое дело в каменный правдой стоят.\n",
      "– Скажи, красная девица, на одной дереве не показывай ваше волость. Как уви \n",
      "----\n",
      "Epoch 1 average loss = 1.1123299837112426\n",
      "<----------------------------------------------------------->\n",
      "Using random seed: по своим домам расходиться, вот и поп поднялся. Пошел старик его провожать, и только вышли на двор –\n",
      "Генерируем текст в пропорциональном режиме temperature = 1.5\n",
      "----\n",
      " по своим домам расходиться, вот и поп поднялся. Пошел старик его провожать, и только вышли на двор – бросился к озеру. Вином сновал Иван – крестьянский сын, царевич пошел на коня и пошел в чужестом поле, как под порог возденный. Вот прилетела царевича в шерсту, увидала его крикнули и давай посадить  \n",
      "----\n",
      "Генерируем текст в пропорциональном режиме temperature = 3\n",
      "----\n",
      " по своим домам расходиться, вот и поп поднялся. Пошел старик его провожать, и только вышли на двор – красные девицы конному приговаривать:\n",
      "– Старик попадил тебя, разговаривайте на меня.\n",
      "– Не бей меня, баба-яга, бабушка, дочерь в зеленых лугам. Послужил тебя с ним да слепой порог отдать – не послушае \n",
      "----\n",
      "Генерируем текст в жадном режиме (наиболее вероятные буквы)\n",
      "----\n",
      " по своим домам расходиться, вот и поп поднялся. Пошел старик его провожать, и только вышли на двор – в ту же минуту послушался и пошел в подзорную трубу и вошел в избу стрелять. В ту пору волос в городе пришел в избу старику и пошел в свое царство. Стал просить царю, в страх привез к своим родной се \n",
      "----\n",
      "Epoch 2 average loss = 1.1069633328914643\n",
      "<----------------------------------------------------------->\n",
      "Using random seed:  явились перед ним, как из земли, два молодца и говорят ему:\n",
      "– Что вам угодно?\n",
      "Солдат удивился и нич\n",
      "Генерируем текст в пропорциональном режиме temperature = 1.5\n",
      "----\n",
      "  явились перед ним, как из земли, два молодца и говорят ему:\n",
      "– Что вам угодно?\n",
      "Солдат удивился и ничего жемает, а перед Иваном места все достанет. А в этом дереве ножится на четвертый день, а кругом сам за своими сестрами. И солдат тотчас же они за собой и пошел в конюшню по смерти и думает. Слышала \n",
      "----\n",
      "Генерируем текст в пропорциональном режиме temperature = 3\n",
      "----\n",
      "  явились перед ним, как из земли, два молодца и говорят ему:\n",
      "– Что вам угодно?\n",
      "Солдат удивился и ничего не было. Все поскакали в свою саблю и спрашивает девушке:\n",
      "– Ну, что хочешь, согласна, теперь сам пришел сокол, да вот какая!\n",
      "– А я в том дороге волосы конь спать, утро вечера мудренее.\n",
      "Пошел он св \n",
      "----\n",
      "Генерируем текст в жадном режиме (наиболее вероятные буквы)\n",
      "----\n",
      "  явились перед ним, как из земли, два молодца и говорят ему:\n",
      "– Что вам угодно?\n",
      "Солдат удивился и ничего не видать – все двенадцать огненным привечим и спрашивает царевна:\n",
      "– Ступай к ней в гостя!\n",
      "– Нет, не видать не возьмет. Как бы тебя не отдам, а в том дворцу постолки на коня и поехали в свое госуд \n",
      "----\n",
      "Epoch 3 average loss = 1.076898809671402\n",
      "<----------------------------------------------------------->\n",
      "Using random seed:  скоро опять нагоню.\n",
      "Повернул назад, отъехал несколько верст и повстречал погоню вдвое больше; переб\n",
      "Генерируем текст в пропорциональном режиме temperature = 1.5\n",
      "----\n",
      "  скоро опять нагоню.\n",
      "Повернул назад, отъехал несколько верст и повстречал погоню вдвое больше; перебрался с неволками, во всем конь поднялся, подходит к Котому морской заяц, и опять потелился отец – красные солнышко заяц воротился с полкими соборок и попадается. Иван-царевич пошел отскатал. В ту пор \n",
      "----\n",
      "Генерируем текст в пропорциональном режиме temperature = 3\n",
      "----\n",
      "  скоро опять нагоню.\n",
      "Повернул назад, отъехал несколько верст и повстречал погоню вдвое больше; перебрался и на том избушке на столе и накрепко прилочили.\n",
      "– Не тре будет тебя из судьба? Кого со мной сидит, и я тебя не узнал.\n",
      "– Ну, ворон, да смотри не прорубить?\n",
      "– Нет, не захотел сюда, сам на моего му \n",
      "----\n",
      "Генерируем текст в жадном режиме (наиболее вероятные буквы)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training ...\")\n",
    "\n",
    "\n",
    "#сколько всего эпох\n",
    "n_epochs=1000\n",
    "\n",
    "# раз в сколько эпох печатать примеры \n",
    "batches_per_epoch = 100\n",
    "\n",
    "#сколько цепочек обрабатывать за 1 вызов функции обучения\n",
    "batch_size=200\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start = np.random.randint(0,len(corpora)-seq_length)\n",
    "    seed_phrase = corpora[start:start+seq_length]\n",
    "    \n",
    "    print (\"<----------------------------------------------------------->\")\n",
    "    print (\"Using random seed:\",seed_phrase)\n",
    "    \n",
    "    print (\"Генерируем текст в пропорциональном режиме temperature = 1.5\")\n",
    "    generate_sample(proportional_sample_fun,seed_phrase, t=1.5) #Попробуй поменять температуру\n",
    "    print (\"Генерируем текст в пропорциональном режиме temperature = 3\")\n",
    "    generate_sample(proportional_sample_fun,seed_phrase, t=3) #Попробуй поменять температуру\n",
    "   # print (\"Using random seed:\",seed_phrase)\n",
    "    print (\"Генерируем текст в жадном режиме (наиболее вероятные буквы)\")\n",
    "    generate_sample(max_sample_fun,seed_phrase)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_random_batches(corpora_ids,batch_size,seq_length)\n",
    "        #print(x.shape, to_categorical(y, nb_classes=len(tokens)).shape)\n",
    "        avg_cost += model.train_on_batch(x, to_categorical(y, num_classes=len(tokens)))[0]\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели\n",
    "\n",
    "В котором вы можете подёргать параметры или вставить свою генерирующую функцию.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конституция нового мирового правительства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "                                                                                             Я пойду наше спил. Тут пришла на двор, словно стали они просить.\n",
      "Иван-царевич еще и говорит:\n",
      "– Чего тебе надобно?\n",
      "– Я – пойдем со мна, слышит ты мой!\n",
      "Вдруг вот тебе старика кости пустила, все поменает и стала \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "seed = \"Я пойду \"\n",
    "sampling_fun = proportional_sample_fun\n",
    "\n",
    "result_length = 200\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length, t=1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "                                  Школьники обязаны заявления и дослЕ сделараются имедельных правомали, уболноми для вступления и после выбор 1006 г.\n",
      " Срок товаров, нет истечениях для обязательства, рупледание уличшего, установленные заключает выборочное причиненные способы обстояния конкретный производстве;\n",
      " 6) обороть вяжимотре и обязанное поваими \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "seed = u\"Школьники обязаны\"\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length, t=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "И далее по списку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
