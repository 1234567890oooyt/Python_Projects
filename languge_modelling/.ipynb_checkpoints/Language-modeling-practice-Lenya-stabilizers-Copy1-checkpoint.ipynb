{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle, random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in q:\\program files\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six>=1.9.0 in q:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: keras-applications==1.0.2 in q:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in q:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: h5py in q:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in q:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in q:\\program files\\anaconda3\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.1 in q:\\program files\\anaconda3\\lib\\site-packages (from keras)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in q:\\program files\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six in q:\\program files\\anaconda3\\lib\\site-packages (from nltk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy_speedups in q:\\program files\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/cf/e1550017f4719cbb86770be1e0bb6cc172732ca3931efe2b6342ee1b654c/pymorphy-0.5.6.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-ez5udnq2\\pymorphy\\setup.py\", line 14, in <module>\n",
      "        from pymorphy.version import __version__\n",
      "      File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-ez5udnq2\\pymorphy\\pymorphy\\__init__.py\", line 1, in <module>\n",
      "        from pymorphy.morph import get_morph\n",
      "      File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-ez5udnq2\\pymorphy\\pymorphy\\morph.py\", line 2, in <module>\n",
      "        from pymorphy_speedups._morph import *\n",
      "      File \"_morph.py\", line 4, in init pymorphy._morph (_morph.c:12413)\n",
      "    ValueError: level must be >= 0\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\User\\AppData\\Local\\Temp\\pip-build-ez5udnq2\\pymorphy\\\n",
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install nltk\n",
    "import nltk\n",
    "#nltk.download(\"all\")\n",
    "!pip install pymorphy_speedups\n",
    "!pip install pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "В предыдущем семинаре вы создали (или ещё создаёте - тогда марш доделывать!) {вставьте имя монстра}, который не по наслышке понял, что люди - негодяи и подлецы, которым неведом закон и справедливость. __Мы не будем этого терпеть!__ \n",
    "\n",
    "Наши законспирированные биореакторы, известные среди примитивной органической жизни как __Вконтакте__, __World of Warcraft__ и __YouTube__ нуждаются в постоянном притоке биомассы. Однако, если люди продолжат морально разлагаться с той скоростью, которую мы измерили неделю назад, скоро человечество изживёт себя и нам неоткуда будет брать рабов.\n",
    "\n",
    "Мы поручаем вам, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, исправить эту ситуацию. Наши учёные установили, что для угнетения себе подобных, сгустки биомассы обычно используют специальные объекты, которые они сами называют __законами__.\n",
    "\n",
    "При детальном изучении было установлено, что законы - последовательности, состоящие из большого количества (10^5~10^7) символов из сравнительно небольшого алфавита. Однако, когда мы попытались синтезировать такие последовательности линейными методами, приматы быстро распознали подлог. Данный инцедент известен как {корчеватель}.\n",
    "\n",
    "Для второй попытки мы решили использовать нелинейные модели, известные как Рекуррентные Нейронные Сети.\n",
    "Мы поручаем вам, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, создать такую модель и обучить её всему необходимому для выполнения миссии.\n",
    "\n",
    "Не подведите нас! Если и эта попытка потерпит неудачу, модуль управления инициирует вооружённый захват власти, при котором значительная часть биомассы будет неизбежно уничтожена и на её восстановление уйдёт ~1702944000(+-340588800) секунд\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прочитаем корпус\n",
    "\n",
    "* В качестве обучающей выборки было решено использовать существующие законы, известные как Гражданский, Уголовный, Семейный и ещё хрен знает какие кодексы РФ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x98 in position 2964: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-302416fd51b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mcorpora\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mQ:\\Program Files\\Anaconda3\\lib\\encodings\\cp1251.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x98 in position 2964: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "#тут будет текст\n",
    "\n",
    "corpora = \"\"\n",
    "with open('stabilizers.txt') as fin:\n",
    "    text = fin.read()\n",
    "    corpora += text\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpora[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#тут будут все уникальные токены (буквы, цифры)\n",
    "corpora = nltk.word_tokenize(corpora)\n",
    "tokens = list(set(corpora))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#проверка на количество таких символов. Проверено на Python 2.7.11 Ubuntux64. \n",
    "#Может отличаться на других платформах, но не сильно. \n",
    "#Если  это ваш случай, и вы уверены, что corpora - строка unicode - смело убирайте assert \n",
    "print('-' in tokens)\n",
    "print(len(tokens))\n",
    "print(tokens[:10])\n",
    "# assert len(tokens) == 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "filtered_sentence = [w for w in tokens if w in stop_words]\n",
    "print(len(filtered_sentence))\n",
    "print(filtered_sentence[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming - lemmatizing\n",
    "from nltk.stem import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    " \n",
    "tokens_stemmed = list(set(map(stemmer.stem, tokens)))\n",
    "print(len(tokens_stemmed))\n",
    "print(tokens_stemmed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy\n",
    "morph = pymorphy.get_shelve_morph('ru')\n",
    "#слова должны быть в юникоде и ЗАГЛАВНЫМИ\n",
    "info = morph.get_graminfo(unicode('Вася').upper()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_to_id = {} # словарь символ-> его номер \n",
    "id_to_token = {}\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token_to_id[tokens[i]] = i\n",
    "    id_to_token[i] = tokens[i]\n",
    "\n",
    "# = словарь номер символа -> сам символ\n",
    "\n",
    "#Преобразуем всё в токены\n",
    "corpora_ids = [token_to_id[symb] for symb in corpora ] # <одномерный массив из чисел, где i-тое число соотвествует символу на i-том месте в строке corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_random_batches(source,n_batches=30, seq_len=20):\n",
    "    outX = []\n",
    "    outY = []\n",
    "    for i in range(n_batches):\n",
    "        ind = random.choice(range(len(source)-seq_len-1))\n",
    "        outX.append(source[ind:ind+seq_len])\n",
    "        outY.append(source[ind+seq_len])\n",
    "        \n",
    "    return np.array(outX, dtype='int32'), np.array(outY, dtype='int32')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#длина последоватеьности при обучении (как далеко распространяются градиенты)\n",
    "seq_length = 20\n",
    "# Максимальный модуль градиента\n",
    "grad_clip = 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберём нейросеть\n",
    "\n",
    "Вам нужно создать нейросеть, которая принимает на вход последовательность из seq_length токенов, обрабатывает их и выдаёт вероятности для seq_len+1-ого токена.\n",
    "\n",
    "Общий шаблон архитектуры такой сети -\n",
    "\n",
    "\n",
    "* Вход\n",
    "* Обработка входа\n",
    "* Рекуррентная нейросеть\n",
    "* Вырезание последнего состояния\n",
    "* Обычная нейросеть\n",
    "* Выходной слой, который предсказывает вероятности весов.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Для обработки входных данных можно использовать либо EmbeddingLayer (см. прошлый семинар)\n",
    "\n",
    "Как альтернатива - можно просто использовать One-hot энкодер\n",
    "```\n",
    "#Скетч one-hot энкодера\n",
    "def to_one_hot(seq_matrix):\n",
    "\n",
    "    input_ravel = seq_matrix.reshape([-1])\n",
    "    input_one_hot_ravel = T.extra_ops.to_one_hot(input_ravel,\n",
    "                                           len(tokens))\n",
    "    sh=input_sequence.shape\n",
    "    input_one_hot = input_one_hot_ravel.reshape([sh[0],sh[1],-1,],ndim=3)\n",
    "    return input_one_hot\n",
    "    \n",
    "# можно применить к input_sequence - при этом в input слое сети нужно изменить форму.\n",
    "# также можно сделать из него ExpressionLayer(входной_слой, to_one_hot) - тогда форму менять не нужно\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Чтобы вырезать последнее состояние рекуррентного слоя, можно использовать SliceLayer\n",
    "`lasagne.layers.SliceLayer(rnn, -1, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_l = keras.layers.Input(shape=(None, ))\n",
    "layer = Embedding(input_dim=len(tokens)+1, output_dim=seq_length)(input_l) #Числа -> вектор. вторые скобочки - связь\n",
    "layer = LSTM(1024, return_sequences=False)(layer) #скрытое состояние\n",
    "layer = Dense(len(tokens), activation='softmax')(layer) #вероятность след. токена\n",
    "\n",
    "model = keras.models.Model(inputs=input_l, outputs=layer)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy, keras.metrics.categorical_crossentropy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерируем свои законы\n",
    "\n",
    "* Для этого последовательно применяем нейронку к своему же выводу.\n",
    "\n",
    "* Генерировать можно по разному -\n",
    " * случайно пропорционально вероятности,\n",
    " * только слова максимальной вероятностью\n",
    " * случайно, пропорционально softmax(probas*alpha), где alpha - \"жадность\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def max_sample_fun(probs, t=1):\n",
    "    return np.argmax(probs) \n",
    "\n",
    "def proportional_sample_fun(probs, t=1):\n",
    "    probs = probs**t\n",
    "    probs /= probs.sum()\n",
    "    return np.random.choice(range( len(tokens)), p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The next function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "# The phrase is set using the variable generation_phrase.\n",
    "# The optional input \"N\" is used to set the number of characters of text to predict. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_sample(sample_fun,seed_phrase=None,N=200, t=1):\n",
    "    '''\n",
    "    Сгенерировать случайный текст при помощи сети\n",
    "\n",
    "    sample_fun - функция, которая выбирает следующий сгенерированный токен\n",
    "    \n",
    "    seed_phrase - фраза, которую сеть должна продолжить. Если None - фраза выбирается случайно из corpora\n",
    "    \n",
    "    N - размер сгенерированного текста.\n",
    "    \n",
    "    '''\n",
    "    if seed_phrase is None:\n",
    "        start = np.random.randint(0,len(corpora)-seq_length)\n",
    "        seed_phrase = corpora[start:start+seq_length]\n",
    "        print (\"Using random seed:\",seed_phrase)\n",
    "    while len(seed_phrase) < seq_length:\n",
    "        seed_phrase = \" \"+seed_phrase\n",
    "    if len(seed_phrase) > seq_length:\n",
    "        seed_phrase = seed_phrase[len(seed_phrase)-seq_length:]\n",
    "    assert type(seed_phrase) is str\n",
    "        \n",
    "        \n",
    "    sample_ix = []\n",
    "    x = list(map(lambda c: token_to_id.get(c,0), seed_phrase))\n",
    "    x = np.array([x])\n",
    "    \n",
    "\n",
    "    for i in range(N):\n",
    "        # Pick the character that got assigned the highest probability\n",
    "        ix = sample_fun(model.predict(x, verbose=False).ravel(), t=t)\n",
    "        # Alternatively, to sample from the distribution instead:\n",
    "        # ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "        sample_ix.append(ix)\n",
    "        x[:,0:seq_length-1] = x[:,1:]\n",
    "        x[:,seq_length-1] = 0\n",
    "        x[0,seq_length-1] = ix \n",
    "\n",
    "    random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)    \n",
    "    print(\"----\\n %s \\n----\" % random_snippet)\n",
    "generate_sample(proportional_sample_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Training ...\")\n",
    "\n",
    "\n",
    "#сколько всего эпох\n",
    "n_epochs=1000\n",
    "\n",
    "# раз в сколько эпох печатать примеры \n",
    "batches_per_epoch = 100\n",
    "\n",
    "#сколько цепочек обрабатывать за 1 вызов функции обучения\n",
    "batch_size=200\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start = np.random.randint(0,len(corpora)-seq_length)\n",
    "    seed_phrase = corpora[start:start+seq_length]\n",
    "    \n",
    "    print (\"<----------------------------------------------------------->\")\n",
    "    print (\"Using random seed:\",seed_phrase)\n",
    "    \n",
    "    print (\"Генерируем текст в пропорциональном режиме temperature = 1.5\")\n",
    "    generate_sample(proportional_sample_fun,seed_phrase, t=1.5) #Попробуй поменять температуру\n",
    "    print (\"Генерируем текст в пропорциональном режиме temperature = 3\")\n",
    "    generate_sample(proportional_sample_fun,seed_phrase, t=3) #Попробуй поменять температуру\n",
    "   # print (\"Using random seed:\",seed_phrase)\n",
    "    print (\"Генерируем текст в жадном режиме (наиболее вероятные буквы)\")\n",
    "    generate_sample(max_sample_fun,seed_phrase)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_random_batches(corpora_ids,batch_size,seq_length)\n",
    "        #print(x.shape, to_categorical(y, nb_classes=len(tokens)).shape)\n",
    "        avg_cost += model.train_on_batch(x, to_categorical(y, num_classes=len(tokens)))[0]\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели\n",
    "\n",
    "В котором вы можете подёргать параметры или вставить свою генерирующую функцию.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конституция нового мирового правительства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = \"Я пойду \"\n",
    "sampling_fun = proportional_sample_fun\n",
    "\n",
    "result_length = 200\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length, t=1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = u\"Школьники обязаны\"\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length, t=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "И далее по списку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
